{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Capacity Metrics (by Kind)\n",
        "by Workspace by Kind by Day\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Capacity_Metrics_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Capacity Metrics** via SemPy DAX execute query function\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value"
      ],
      "metadata": {},
      "id": "d640e2d5-03e4-4147-98c7-d4591d30283b"
    },
    {
      "cell_type": "code",
      "source": [
        "import sempy.fabric as fabric\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "ea04a53c-a7b0-4dc1-a67f-626628d77ad9"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "# These parameters will be overwritten while executing the notebook \n",
        "# from Load_FUAM_Data_E2E Pipeline\n",
        "metric_days_in_scope = 3\n",
        "metric_workspace = \"0865c010-c5ba-4279-b2fc-51d85a369983\"\n",
        "metric_dataset = \"f8eddd0b-dfa4-4830-87a1-79b51bff6c53\"\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "tags": [
          "parameters"
        ]
      },
      "id": "dcc2d0df-5297-441c-9df2-84686682f755"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.capacity_metrics_by_item_kind_by_day_silver\"\n",
        "gold_table_name = \"capacity_metrics_by_item_kind_by_day\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\""
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "5e0bc19e-6685-4eb4-bc97-1a6b83afc5f4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Table Status\n",
        "\n",
        "# Go with primary\n",
        "primary_version_active = True\n",
        "try:\n",
        "    check_table_structure_query = \"\"\"EVALUATE SELECTCOLUMNS( TOPN(1, 'Metrics By Item and Day'), 'Metrics By Item and Day'[Date])\"\"\"\n",
        "    check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "except:\n",
        "    primary_version_active = False\n",
        "\n",
        "# Try secondary\n",
        "if primary_version_active == False:\n",
        "    try:\n",
        "        check_table_structure_query_alternative = \"\"\"EVALUATE SELECTCOLUMNS( TOPN(1, 'MetricsByItemandDay'), 'MetricsByItemandDay'[Date])\"\"\"\n",
        "        check_table_structure_df_alternative = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query_alternative)\n",
        "    except:\n",
        "        primary_version_active = None\n",
        "        print(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")\n",
        "        exit"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "b8541da8-f6e1-4537-8efa-d579bccc4367"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    if primary_version_active == True:\n",
        "        print(\"INFO: Primary version is compatible.\")\n",
        "    elif primary_version_active == False:\n",
        "        print(\"INFO: Secondary version is compatible.\")\n",
        "    else:\n",
        "        print(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "3d75dbdf-f3d4-4cd4-9aff-a8a9bdf3ae14"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch capacities from connected capacity metrics app\n",
        "if primary_version_active == True:\n",
        "  capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities,  \"capacity Id\", Capacities[capacity Id] , \"state\" , Capacities[state] )\"\"\"\n",
        "else:\n",
        "  capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities,  \"capacity Id\", Capacities[CapacityId] , \"state\" , Capacities[state] )\"\"\"\n",
        "capacities = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=capacity_query)\n",
        "capacities.columns = ['CapacityId', 'State']\n",
        "capacities = spark.createDataFrame(capacities)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "5f24dea6-fa0a-4692-92c0-32de283c47a1"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    display(capacities)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "cellStatus": "",
        "collapsed": false,
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "4e1334ef-5f7c-4444-8651-fef4aa72f0f3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate days\n",
        "def iterate_dates(start_date, end_date):\n",
        "    # Init array\n",
        "    dates = []\n",
        "    # Convert string inputs to datetime objects\n",
        "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    \n",
        "    # Initialize current date as start date\n",
        "    current_date = start.date()\n",
        "    \n",
        "    while current_date <= end.date():\n",
        "\n",
        "        dates.append(\n",
        "            {\n",
        "                \"date\": current_date,\n",
        "                \"year\": current_date.year,\n",
        "                \"month\": current_date.month,\n",
        "                \"day\": current_date.day\n",
        "            })\n",
        "        # Move to the next day\n",
        "        current_date += dt.timedelta(days=1)\n",
        "\n",
        "    return dates"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "959509d6-4647-427c-90ae-b4edab474e4d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Silver table\n",
        "try:\n",
        "    query = \"DELETE FROM \" + silver_table_name\n",
        "    spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(\"Silver table doesn't exist yet.\") "
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "aaae3981-7e98-4481-9441-c4a946412409"
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate capacities and days\n",
        "for cap in capacities.collect():\n",
        "    capacity_id = cap[0]\n",
        "    \n",
        "    print(f\"INFO: Scoped CapacityId: {capacity_id}\")\n",
        "\n",
        "    try:\n",
        "        # Get today's date\n",
        "        today = datetime.now()\n",
        "\n",
        "        # Calculate the dates between today and days_in_scope\n",
        "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
        "\n",
        "        # Format dates in 'yyyy-mm-dd'\n",
        "        today_str = today.strftime('%Y-%m-%d')\n",
        "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
        "\n",
        "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
        "\n",
        "        # Iterate days for current capacity\n",
        "        for date in date_array:\n",
        "\n",
        "            year = date['year']\n",
        "            month = date['month']\n",
        "            day = date['day']\n",
        "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
        "            print(f\"INFO: Get data for CapacityId: {capacity_id}\")\n",
        "\n",
        "            dax_query_primary = f\"\"\"\n",
        "                DEFINE \n",
        "                    MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('Metrics By Item and Day'[Date])),\n",
        "                                            'Metrics By Item and Day'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                    SUMMARIZECOLUMNS(\n",
        "                                            Capacities[capacity Id],\n",
        "                                            Items[Workspace Id],\n",
        "                                            'Metrics By Item and Day'[Date],\n",
        "                                            'Items'[Item Kind],\n",
        "                                            FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                                            __DS0FilterTable,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"S_Dur\", SUM('Metrics By Item Operation And Day'[Duration (s)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"S_CU\", SUM('Metrics By Item Operation And Day'[CU (s)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"TH_M\", SUM('Metrics By Item Operation And Day'[Throttling (min)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_U\", SUM('Metrics By Item Operation And Day'[Users]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_SO\", SUM('Metrics By Item Operation And Day'[Successful operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_RO\", SUM('Metrics By Item Operation And Day'[Rejected operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_O\", SUM('Metrics By Item Operation And Day'[Operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_IO\", SUM('Metrics By Item Operation And Day'[Invalid operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_FO\", SUM('Metrics By Item Operation And Day'[Failed operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_CO\", SUM('Metrics By Item Operation And Day'[Cancelled operations])\n",
        "                                            )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            dax_query_alternative = f\"\"\"\n",
        "                DEFINE \n",
        "                    MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('MetricsByItemandDay'[Date])),\n",
        "                                            'MetricsByItemandDay'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                    SUMMARIZECOLUMNS(\n",
        "                                            Capacities[capacityId],\n",
        "                                            Items[WorkspaceId],\n",
        "                                            'MetricsByItemandDay'[Date],\n",
        "                                            'Items'[ItemKind],\n",
        "                                            FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
        "                                            __DS0FilterTable,\n",
        "                                            \"S_Dur\", SUM('MetricsByItemandDay'[sum_duration]),\n",
        "                                            \"S_CU\", SUM('MetricsByItemandDay'[sum_CU]),\n",
        "                                            \"TH_M\", SUM('MetricsByItemandDay'[Throttling (min)]),\n",
        "                                            \"C_U\", SUM('MetricsByItemandDay'[count_users]),\n",
        "                                            \"C_SO\", SUM('MetricsByItemandDay'[count_successful_operations]),\n",
        "                                            \"C_RO\", SUM('MetricsByItemandDay'[count_rejected_operations]),\n",
        "                                            \"C_O\", SUM('MetricsByItemandDay'[count_operations]),\n",
        "                                            \"C_IO\", SUM('MetricsByItemandDay'[count_Invalid_operations]),\n",
        "                                            \"C_FO\", SUM('MetricsByItemandDay'[count_failure_operations]),\n",
        "                                            \"C_CO\", SUM('MetricsByItemandDay'[count_cancelled_operations])\n",
        "                                            )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC\n",
        "                    \"\"\"\n",
        "\n",
        "            dax_query = \"\"\n",
        "            # Choose query\n",
        "            if primary_version_active == True:\n",
        "                dax_query = dax_query_primary\n",
        "            else:\n",
        "                dax_query = dax_query_alternative\n",
        "\n",
        "            # Execute DAX query\n",
        "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
        "            capacity_df.columns = ['CapacityId', 'WorkspaceId', 'Date',  \n",
        "                                    'ItemKind', 'DurationInSec','TotalCUs', 'ThrottlingInMin', \n",
        "                                    'UserCount','SuccessOperationCount', 'RejectedOperationCount','OperationCount',\n",
        "                                    'InvalidOperationCount','FailureOperationCount','CancelledOperationCount', 'DateKey']\n",
        "            \n",
        "            if not(capacity_df.empty):\n",
        "                # Transfer pandas df to spark df\n",
        "                capacity_df = spark.createDataFrame(capacity_df)\n",
        "\n",
        "                if display_data:\n",
        "                    display(capacity_df)\n",
        "\n",
        "                # Write prepared bronze_df to silver delta table\n",
        "                print(f\"INFO: Appending data. Capacity: {capacity_id} on Date: {date_label}\")\n",
        "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name)\n",
        "            else:\n",
        "                print(f\"INFO: No data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        print('ERROR: Exception for CapacityId: ' + capacity_id + '. ->' + str(ex))\n",
        "        continue"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "cellStatus": "",
        "collapsed": false,
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "de5386ef-b266-48fb-9369-b153da0d0c04"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Silver table data\n",
        "query = \"SELECT * FROM \" + silver_table_name\n",
        "silver_df = spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "4ac27758-0175-46eb-9b68-d996b908bf76"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"INFO: Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_df.alias('s'),\n",
        "        \"s.CapacityId = t.CapacityId AND s.WorkspaceId = t.WorkspaceId AND s.Date = t.Date AND s.ItemKind = t.ItemKind\"\n",
        "    ) \\\n",
        "    .whenMatchedUpdate(set =\n",
        "        {\n",
        "             \"DurationInSec\": \"s.DurationInSec\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
        "            ,\"UserCount\": \"s.UserCount\"\n",
        "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
        "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
        "            ,\"OperationCount\": \"s.OperationCount\"\n",
        "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
        "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
        "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
        "        }\n",
        "    ) \\\n",
        "    .whenNotMatchedInsert(values =\n",
        "        {\n",
        "             \"CapacityId\": \"s.CapacityId\"\n",
        "            ,\"WorkspaceId\": \"s.WorkspaceId\"\n",
        "            ,\"Date\": \"s.Date\"\n",
        "            ,\"ItemKind\": \"s.ItemKind\"\n",
        "            ,\"DurationInSec\": \"s.DurationInSec\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
        "            ,\"UserCount\": \"s.UserCount\"\n",
        "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
        "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
        "            ,\"OperationCount\": \"s.OperationCount\"\n",
        "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
        "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
        "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
        "            ,\"DateKey\": \"s.DateKey\"\n",
        "        }\n",
        "    ) \\\n",
        "    .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"INFO: Gold table will be created.\")\n",
        "\n",
        "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "7bb0ff85-ab07-4f5e-bf5e-49475e7f2be7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Silver table\n",
        "query = \"DELETE FROM \" + silver_table_name\n",
        "spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "3971a614-bc18-4c97-9981-3e43b20ab119"
    }
  ],
  "metadata": {
    "a365ComputeOptions": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "sessionKeepAliveTimeout": 0,
    "widgets": {},
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "environment": {},
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d",
        "known_lakehouses": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}