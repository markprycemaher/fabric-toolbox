{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Delegated Tenant Settings Overrides\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: lightgreen;\">**APPEND**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Delegated_Tenant_Settings_Override_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Files** from FUAM_Lakehouse folder **bronze_file_location** variable\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value"
      ],
      "metadata": {},
      "id": "d04ff9bd-ff98-425a-8599-2ae26173b210"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.functions import col, explode, from_json\n",
        "from delta.tables import *\n",
        "import pandas as pd\n",
        "import json"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "ec6cd04a-8a2c-47e2-96cb-ea732bc81e12"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "74bf5983-f2fd-4b91-a880-1cab65d9a0ab"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "bronze_file_location = f\"Files/raw/delegated_tenant_settings_overrides/\"\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.delegated_tenant_settings_overrides_silver\"\n",
        "gold_table_name = \"delegated_tenant_settings_overrides\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "48c73262-1d2a-4f14-99bf-4dfca2146d4d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Silver table, if exists\n",
        "if spark.catalog.tableExists(silver_table_name):\n",
        "    del_query = \"DELETE FROM \" + silver_table_name\n",
        "    spark.sql(del_query)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "33a3e5d2-1257-407c-b046-0c65f8bec3d4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Bronze data\n",
        "bronze_df = spark.read.option(\"multiline\", \"true\").json(bronze_file_location)\n",
        "\n",
        "if display_data:\n",
        "    display(bronze_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "0e2ce793-4613-4374-bb4c-b89847b86b7f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode json subset structure\n",
        "try:\n",
        "    exploded_df = bronze_df.select(explode(\"Overrides\").alias(\"d\"))\n",
        "except:\n",
        "    exploded_df = bronze_df.select(explode(\"value\").alias(\"d\"))\n",
        "\n",
        "if display_data:\n",
        "    display(exploded_df)\n",
        "\n",
        "# This prevents the notebook running into an error when no delegated tenant settings are existant in the tenant\n",
        "if exploded_df.count() == 0 :\n",
        "    notebookutils.notebook.exit(\"No Delegated Settings available\")    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "1f9f44a0-b163-4ba7-9009-269dc76631e9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract json objects to tabular form\n",
        "tenantSettings_df = exploded_df.select(col(\"d.id\").alias(\"OverrideId\"), explode(\"d.tenantSettings\").alias(\"ts\"))\n",
        "\n",
        "if display_data:\n",
        "    display(tenantSettings_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "038fdf73-53cc-443d-8eb0-f79717365912"
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop session in case no delegated settings are available, so no error is raised\n",
        "if tenantSettings_df.count() == 0:\n",
        "    print(\"Stop session to prevent error in case no delegated settings are existant\")\n",
        "    mssparkutils.session.stop()\n",
        "\n",
        "else:\n",
        "\n",
        "    # Select columns from df\n",
        "    silver_df = tenantSettings_df.select(\n",
        "        col(\"OverrideId\"),\n",
        "        col(\"OverrideId\").alias(\"CapacityId\"),\n",
        "        col(\"ts.tenantSettingGroup\"),\n",
        "        col(\"ts.title\"),\n",
        "        col(\"ts.delegatedFrom\"),\n",
        "        col(\"ts.settingName\"),\n",
        "        col(\"ts.enabled\"),\n",
        "        col(\"ts.canSpecifySecurityGroups\")\n",
        "        )\n",
        "\n",
        "    # Show data for debug\n",
        "    if display_data:\n",
        "        display(silver_df)\n",
        "\n",
        "    # Write prepared bronze_df to silver delta table\n",
        "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name)\n",
        "\n",
        "    # Get Silver table data\n",
        "    query = \"\"\"\n",
        "    SELECT \n",
        "        to_date(current_timestamp()) AS TransferDate\n",
        "        ,current_timestamp() AS TransferDateTime\n",
        "        ,*\n",
        "    FROM \"\"\" + silver_table_name\n",
        "\n",
        "    silver_df = spark.sql(query)\n",
        "\n",
        "    # Show data for debug\n",
        "    if display_data:\n",
        "        display(silver_df)\n",
        "\n",
        "    # Append Gold Lakehouse table\n",
        "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)\n",
        "\n",
        "    # Write history of bronze files\n",
        "    mssparkutils.fs.cp(bronze_file_location, bronze_file_location.replace(\"Files/raw/\", \"Files/history/\") + datetime.now().strftime('%Y/%m/%d') + \"/\", True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "bcf833e5-2a1c-4f10-93aa-41e529141706"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d",
        "known_lakehouses": []
      },
      "environment": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}