{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Activities\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: lightgreen;\">**APPEND**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Activities_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Files** from FUAM_Lakehouse folder **bronze_file_location** variable\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value"
      ],
      "metadata": {},
      "id": "3f540cb9-3440-4fc6-be14-d2310ed64767"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode, to_date, date_format, lit, upper\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *\n",
        "import datetime"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "846bb0c9-efcd-42fa-b785-2d5c44f60291"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "20315deb-201b-4583-8ad9-4e65a2752997"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "bronze_file_location = f\"Files/raw/activities/*/\"\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.activities_silver\"\n",
        "gold_table_name = \"activities\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\"\n",
        "\n",
        "last_activity_date = ''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "afa6751c-515f-4655-8edc-ec2cd286f09e"
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts all complex data types to StringType\n",
        "def convert_columns_to_string(schema, parent = \"\", lvl = 0):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - schema: Dataframe schema as StructType\n",
        "    \n",
        "    Output: List\n",
        "    Returns a list of columns in the schema casting them to String to use in a selectExpr Spark function.\n",
        "    \"\"\"\n",
        "    \n",
        "    lst=[]\n",
        "    \n",
        "    for x in schema:\n",
        "        # check if complex datatype has to be converted to string\n",
        "        if str(x.dataType) in {\"DateType()\", \"StringType()\", \"BooleanType()\", \"LongType()\", \"IntegerType()\", \"DoubleType()\", \"FloatType()\"}:\n",
        "            # no need to convert\n",
        "            lst.append(\"{col}\".format(col=x.name))\n",
        "        else:\n",
        "            # it has to be converted\n",
        "            # print(str(x.dataType))\n",
        "            lst.append(\"cast({col} as string) as {col}\".format(col=x.name))\n",
        "\n",
        "    return lst"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "d6813d56-b1bf-43b8-8171-990415a47c24"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Bronze data\n",
        "bronze_df = spark.read.option(\"multiline\", \"true\").json(bronze_file_location)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "2614fcb9-64d5-4d89-b3b6-271802d0b03f"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    display(bronze_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "0870966f-5a41-43ad-8e51-2f77ee33edc5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode json subset structure\n",
        "exploded_df = bronze_df.select(explode(\"activityEventEntities\").alias(\"d\"))\n",
        "\n",
        "del bronze_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "24879f76-46f3-41dc-b57a-f0c911c20b4f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all columns (columns are dynamic)\n",
        "silver_df = exploded_df.select(\n",
        "    to_date(col(\"d.CreationTime\").substr(1,10), \"yyyy-MM-dd\").alias(\"CreationDate\"),\n",
        "    date_format(\"d.CreationTime\",\"yyyyMMdd\").alias(\"CreationDateKey\"),\n",
        "    date_format(\"d.CreationTime\",\"H\").alias(\"CreationHour\"),\n",
        "    date_format(\"d.CreationTime\",\"mm\").alias(\"CreationMinute\"),\n",
        "    col(\"d.*\")\n",
        "    )\n",
        "# Put selected ID columns to Upper Case\n",
        "for co in silver_df.columns:\n",
        "    if co in ['ActivityId','ArtifactId','CapacityId','DashboardId','DataflowId','DatasetId','DatasourceId','FolderObjectId','GatewayId','Id','ItemId','ReportId','UserId','WorkspaceId','','','','','','','','','',]:\n",
        "        silver_df = silver_df.withColumn(co, f.upper(silver_df[co]))\n",
        "del exploded_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "1252f3e3-7bc7-4bc0-a381-aa8f70cb428b"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    display(silver_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "0178b8e7-be3a-4a7f-b6a1-9c3660a30a8d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if gold table exists \n",
        "table_exists = None\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
        "    table_exists = True\n",
        "    print(\"Gold table exists.\")\n",
        "else:\n",
        "    table_exists = False\n",
        "\n",
        "# Get latest activity date from silver_df\n",
        "silver_min_df = silver_df.select(col('CreationDate')).orderBy(col('CreationDate'), ascending=True).first()\n",
        "silver_last_activity_date = silver_min_df['CreationDate']\n",
        "\n",
        "# Calculate latest activity date\n",
        "if table_exists:\n",
        "\n",
        "    # Get latest activity date from gold table\n",
        "    get_latest_date_sql = \"SELECT CreationDate FROM FUAM_Lakehouse.activities ORDER BY CreationDate DESC LIMIT 1\"\n",
        "    gold_min_df = spark.sql(get_latest_date_sql)\n",
        "    if gold_min_df.count() == 0:\n",
        "        # in case there are no records in gold take silver_last_activity_date\n",
        "        print(\"No existing records\")\n",
        "        gold_last_activity_date = silver_last_activity_date\n",
        "    else:\n",
        "        gold_last_activity_date = gold_min_df.first()['CreationDate']\n",
        "\n",
        "    if silver_last_activity_date < gold_last_activity_date:\n",
        "        print(\"From silver_df\")\n",
        "        last_activity_date = silver_last_activity_date\n",
        "    else:\n",
        "        print(\"From gold\")\n",
        "        last_activity_date = gold_last_activity_date\n",
        "\n",
        "else:\n",
        "    print(\"From silver_df\")\n",
        "    last_activity_date = silver_last_activity_date\n",
        "\n",
        "print(last_activity_date)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "727cd7d7-d8f4-4c84-9b12-152283122e1d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean delta content from Gold table\n",
        "if table_exists:\n",
        "    del_query = f\"DELETE FROM FUAM_Lakehouse.{gold_table_name} WHERE CreationDate >= TO_DATE('{last_activity_date}')\"\n",
        "    spark.sql(del_query)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c25dbe5c-969d-40af-8e2f-bb17f7a667f8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter silver_df data based on last activity date\n",
        "silver_df = silver_df.filter(f.col(\"CreationDate\") >= f.lit(last_activity_date))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c1fc747b-0c00-4bdd-aaa2-d344dd6be4b5"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    display(silver_df)\n",
        "    # show converted table schema\n",
        "    print(convert_columns_to_string(silver_df.schema))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "85023807-4305-4eb1-98f2-706284150b3f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert silver_df's complex data type columns to StringType columns\n",
        "silver_df_converted = silver_df.selectExpr(convert_columns_to_string(silver_df.schema))\n",
        "del silver_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7b457628-7333-4de7-96c1-33dd5b2606b3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Write prepared silver_df_converted to gold delta table\n",
        "silver_df_converted.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "ffcfd295-d567-465c-8ff7-e4a457ee6b40"
    },
    {
      "cell_type": "code",
      "source": [
        "# write history of bronze files\n",
        "path = bronze_file_location.replace(\"*/\", '', )\n",
        "mssparkutils.fs.cp(path, path.replace(\"Files/raw/\", \"Files/history/\") + datetime.datetime.now().strftime('%Y/%m/%d') + \"/\", True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "3fa017d6-c3fc-4c08-bae9-55065ed1ac0c"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "ceefb497-a68f-49b3-89fe-7161809ebb7e"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d",
        "known_lakehouses": []
      },
      "environment": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}