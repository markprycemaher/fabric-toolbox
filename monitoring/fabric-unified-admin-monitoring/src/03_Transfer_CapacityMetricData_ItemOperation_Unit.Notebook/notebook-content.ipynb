{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Capacity Metrics (by Item by Operation)\n",
        "by Workspace by Kind by Item by Operation by Day\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Capacity_Metrics_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Capacity Metrics** via SemPy DAX execute query function\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value\n"
      ],
      "metadata": {},
      "id": "eb45896e-9773-4d41-8c08-7b66409f64f3"
    },
    {
      "cell_type": "code",
      "source": [
        "import sempy.fabric as fabric\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c3aa73d3-30ba-4b2c-8b3e-7902a84e14f4"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "# These parameters will be overwritten while executing the notebook \n",
        "# from Load_FUAM_Data_E2E Pipeline\n",
        "metric_days_in_scope = 3\n",
        "metric_workspace = \"43e58404-650a-4fd3-b6e6-e3e0039e4a7e\"\n",
        "metric_dataset = \"47506f43-f977-4b91-bf2e-c666f659d50d\"\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "32769254-61c2-49c6-9d1a-b9deace1672a"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.capacity_metrics_by_item_by_operation_by_day_silver\"\n",
        "gold_table_name = \"capacity_metrics_by_item_by_operation_by_day\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\""
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c4dc4f08-4c44-4bbf-a520-416b2d1677af"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Table Status\n",
        "\n",
        "# Go with primary\n",
        "primary_version_active = True\n",
        "try:\n",
        "    check_table_structure_query = \"\"\"EVALUATE SELECTCOLUMNS( TOPN(1, 'Metrics By Item Operation And Day'), 'Metrics By Item Operation And Day'[Date])\"\"\"\n",
        "    check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "except:\n",
        "    primary_version_active = False\n",
        "\n",
        "# Try secondary\n",
        "if primary_version_active == False:\n",
        "    try:\n",
        "        check_table_structure_query_alternative = \"\"\"EVALUATE SELECTCOLUMNS( TOPN(1, 'MetricsByItemandOperationandDay'), 'MetricsByItemandOperationandDay'[Date])\"\"\"\n",
        "        check_table_structure_df_alternative = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query_alternative)\n",
        "    except:\n",
        "        primary_version_active = None\n",
        "        print(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")\n",
        "        exit"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c735d0ab-cc6a-4099-9596-c64ae41d72d9"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    if primary_version_active == True:\n",
        "        print(\"INFO: Primary version is compatible.\")\n",
        "    elif primary_version_active == False:\n",
        "        print(\"INFO: Secondary version is compatible.\")\n",
        "    else:\n",
        "        print(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "69c9fa67-1a4d-41a5-9ff6-dea5b2eed8eb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch capacities from connected capacity metrics app\n",
        "if primary_version_active == True:\n",
        "  capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[capacity Id] , \"state\" , Capacities[state] )\"\"\"\n",
        "else:\n",
        "  capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[CapacityId] , \"state\" , Capacities[state] )\"\"\"\n",
        "capacities = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=capacity_query)\n",
        "capacities.columns = ['CapacityId', 'State']\n",
        "capacities = spark.createDataFrame(capacities)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7f45ab4d-77a8-4e3e-b447-60080e961895"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    display(capacities)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "id": "da6f2a2d-5b97-458e-998e-74c9a9979c53"
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate days\n",
        "def iterate_dates(start_date, end_date):\n",
        "    # Init array\n",
        "    dates = []\n",
        "    # Convert string inputs to datetime objects\n",
        "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    \n",
        "    # Initialize current date as start date\n",
        "    current_date = start.date()\n",
        "    \n",
        "    while current_date <= end.date():\n",
        "\n",
        "        dates.append(\n",
        "            {\n",
        "                \"date\": current_date,\n",
        "                \"year\": current_date.year,\n",
        "                \"month\": current_date.month,\n",
        "                \"day\": current_date.day\n",
        "            })\n",
        "        # Move to the next day\n",
        "        current_date += dt.timedelta(days=1)\n",
        "\n",
        "    return dates"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "ff88aaa6-f73b-4899-8f06-bdf505744ffa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Silver table\n",
        "try:\n",
        "    query = \"DELETE FROM \" + silver_table_name\n",
        "    spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(\"INFO: Silver table doesn't exist yet.\") "
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "76ade6a9-b811-492a-816e-1d6f74ab426c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate workspaces and days\n",
        "for ca in capacities.collect():\n",
        "    capacity_id = ca[0]\n",
        "\n",
        "    print(f\"INFO: Scoped CapacityId: {capacity_id}\")\n",
        "\n",
        "    try:\n",
        "        # Get today's date\n",
        "        today = datetime.now()\n",
        "\n",
        "        # Calculate the dates between today and days_in_scope\n",
        "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
        "\n",
        "        # Format dates in 'yyyy-mm-dd'\n",
        "        today_str = today.strftime('%Y-%m-%d')\n",
        "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
        "\n",
        "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
        "\n",
        "        # Iterate days for current capacity\n",
        "        for date in date_array:\n",
        "            year = date['year']\n",
        "            month = date['month']\n",
        "            day = date['day']\n",
        "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
        "            print(f\"INFO: Get data for CapacityId: {capacity_id}\")\n",
        "\n",
        "            dax_query_primary = f\"\"\"\n",
        "                DEFINE\n",
        "                    MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),\n",
        "                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                SUMMARIZECOLUMNS(\n",
        "                                        Capacities[capacity Id],\n",
        "                                        Items[Workspace Id],\n",
        "                                        'Metrics By Item Operation And Day'[Date],\n",
        "                                        'Metrics By Item Operation And Day'[Item Id],\n",
        "                                        'Items'[Item Kind],\n",
        "                                        'Metrics By Item Operation And Day'[Operation name],\n",
        "                                        FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                                        __DS0FilterTable,\n",
        "                                        \"S_Dur\", SUM('Metrics By Item Operation And Day'[Duration (s)]),\n",
        "                                        \"S_CU\", SUM('Metrics By Item Operation And Day'[CU (s)]),\n",
        "                                        \"TH_M\", SUM('Metrics By Item Operation And Day'[Throttling (min)]),\n",
        "                                        \"C_U\", SUM('Metrics By Item Operation And Day'[Users]),\n",
        "                                        \"C_SO\", SUM('Metrics By Item Operation And Day'[Successful operations]),\n",
        "                                        \"C_RO\", SUM('Metrics By Item Operation And Day'[Rejected operations]),\n",
        "                                        \"C_O\", SUM('Metrics By Item Operation And Day'[Operations]),\n",
        "                                        \"C_IO\", SUM('Metrics By Item Operation And Day'[Invalid operations]),\n",
        "                                        \"C_FO\", SUM('Metrics By Item Operation And Day'[Failed operations]),\n",
        "                                        \"C_CO\", SUM('Metrics By Item Operation And Day'[Cancelled operations])\n",
        "                                        )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC\n",
        "     \n",
        "            \"\"\"\n",
        "\n",
        "            dax_query_alternative = f\"\"\"\n",
        "                DEFINE\n",
        "                    MPARAMETER 'CapacityID' =  \\\"{capacity_id}\\\"\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('MetricsByItemandOperationandDay'[Date])),\n",
        "                                            'MetricsByItemandOperationandDay'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                SUMMARIZECOLUMNS(\n",
        "                                        Capacities[capacityId],\n",
        "                                        Items[WorkspaceId],\n",
        "                                        'MetricsByItemandOperationandDay'[Date],\n",
        "                                        'MetricsByItemandOperationandDay'[ItemId],\n",
        "                                        'Items'[ItemKind],\n",
        "                                        'MetricsByItemandOperationandDay'[OperationName],\n",
        "                                        FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
        "                                        __DS0FilterTable,\n",
        "                                        \"S_Dur\", SUM('MetricsByItemandOperationandDay'[sum_duration]),\n",
        "                                        \"S_CU\", SUM('MetricsByItemandOperationandDay'[sum_CU]),\n",
        "                                        \"TH_M\", SUM('MetricsByItemandOperationandDay'[Throttling (min)]),\n",
        "                                        \"C_U\", SUM('MetricsByItemandOperationandDay'[count_users]),\n",
        "                                        \"C_SO\", SUM('MetricsByItemandOperationandDay'[count_successful_operations]),\n",
        "                                        \"C_RO\", SUM('MetricsByItemandOperationandDay'[count_rejected_operations]),\n",
        "                                        \"C_O\", SUM('MetricsByItemandOperationandDay'[count_operations]),\n",
        "                                        \"C_IO\", SUM('MetricsByItemandOperationandDay'[count_Invalid_operations]),\n",
        "                                        \"C_FO\", SUM('MetricsByItemandOperationandDay'[count_failure_operations]),\n",
        "                                        \"C_CO\", SUM('MetricsByItemandOperationandDay'[count_cancelled_operations])\n",
        "                                        )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC      \n",
        "            \"\"\"\n",
        "            \n",
        "\n",
        "            dax_query = \"\"\n",
        "            # Choose query\n",
        "            if primary_version_active == True:\n",
        "                dax_query = dax_query_primary\n",
        "            else:\n",
        "                dax_query = dax_query_alternative\n",
        "\n",
        "            # Execute DAX query\n",
        "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
        "            capacity_df.columns = [ 'CapacityId', 'WorkspaceId', 'Date', \n",
        "                                    'ItemId', 'ItemKind', 'OperationName', 'DurationInSec',\n",
        "                                    'TotalCUs', 'ThrottlingInMin', 'UserCount','SuccessOperationCount', \n",
        "                                    'RejectedOperationCount','OperationCount','InvalidOperationCount',\n",
        "                                    'FailureOperationCount','CancelledOperationCount', 'DateKey'\n",
        "                                    ]\n",
        "            if not(capacity_df.empty):\n",
        "                # Transfer pandas df to spark df\n",
        "                capacity_df = spark.createDataFrame(capacity_df)\n",
        "\n",
        "                # Write data in Lakehouse\n",
        "                print(f\"INFO: Appending data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name) \n",
        "            else:\n",
        "                print(f\"INFO: No data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        print('ERROR: Exception for CapacityId: ' + capacity_id + '. ->' + str(ex))\n",
        "        continue\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c7296361-8c10-4d26-a1ed-f99a47d60b44"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Silver table data\n",
        "query = \"SELECT * FROM \" + silver_table_name\n",
        "silver_df = spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7a5006ec-2a2f-4c7a-8563-e1badea4d3a0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"INFO: Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_df.alias('s'),\n",
        "        \"s.CapacityId = t.CapacityId AND s.WorkspaceId = t.WorkspaceId AND s.Date = t.Date AND s.ItemId = t.ItemId AND s.ItemKind = t.ItemKind AND s.OperationName = t.OperationName\"\n",
        "    ) \\\n",
        "    .whenMatchedUpdate(set =\n",
        "        {\n",
        "             \"DurationInSec\": \"s.DurationInSec\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
        "            ,\"UserCount\": \"s.UserCount\"\n",
        "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
        "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
        "            ,\"OperationCount\": \"s.OperationCount\"\n",
        "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
        "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
        "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
        "        }\n",
        "    ) \\\n",
        "    .whenNotMatchedInsert(values =\n",
        "        {\n",
        "             \"CapacityId\": \"s.CapacityId\"\n",
        "            ,\"WorkspaceId\": \"s.WorkspaceId\"\n",
        "            ,\"Date\": \"s.Date\"\n",
        "            ,\"ItemId\": \"s.ItemId\"\n",
        "            ,\"ItemKind\": \"s.ItemKind\"\n",
        "            ,\"OperationName\": \"s.OperationName\"\n",
        "            ,\"DurationInSec\": \"s.DurationInSec\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
        "            ,\"UserCount\": \"s.UserCount\"\n",
        "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
        "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
        "            ,\"OperationCount\": \"s.OperationCount\"\n",
        "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
        "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
        "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
        "            ,\"DateKey\": \"s.DateKey\"\n",
        "        }\n",
        "    ) \\\n",
        "    .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"INFO: Gold table will be created.\")\n",
        "\n",
        "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "2cb48c88-62e9-4cb0-b8e9-932900433c2d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Silver table\n",
        "query = \"DELETE FROM \" + silver_table_name\n",
        "spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "37a20095-46ec-4a54-8615-74ae1e2edf67"
    }
  ],
  "metadata": {
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "sessionKeepAliveTimeout": 0,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "a365ComputeOptions": null,
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d"
      },
      "environment": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}