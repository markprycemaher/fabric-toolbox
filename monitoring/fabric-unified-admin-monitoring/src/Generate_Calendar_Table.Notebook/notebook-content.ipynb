{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, sequence, to_date\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, weekofyear, date_format, to_date, expr"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "3e45aa6b-fc21-46a5-a0e1-88b1189dcb09"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter\n",
        "beginDate = '2024-01-01'\n",
        "endDate = '2030-12-31'\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "6b71aab0-90c2-4431-a640-4c15c3fd78ac"
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.sql(f\"select explode(sequence(to_date('{beginDate}'), to_date('{endDate}'), interval 1 day)) as date\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "aee91875-bce5-488d-938b-0f08e7cff40c"
    },
    {
      "cell_type": "code",
      "source": [
        "date_df = df.select(\n",
        "    date_format(\"date\",\"yyyyMMdd\").alias(\"DateKey\"),\n",
        "    date_format(\"date\",\"yyyy-MM-dd\").alias(\"Date\"),\n",
        "    col(\"date\").alias(\"Date2Key\"),\n",
        "    year(\"date\").alias(\"Year\"),\n",
        "    month(\"date\").alias(\"Month\"),\n",
        "    dayofmonth(\"date\").alias(\"Day\"),\n",
        "    weekofyear(\"date\").alias(\"WeekOfYear\"),\n",
        "    date_format(\"date\",\"yyyy-MM\").alias(\"YearMonth\"),\n",
        "    date_format(\"date\", \"E\").alias(\"DayOfWeek\")\n",
        ")\n",
        "\n",
        "date_df = date_df.createOrReplaceTempView('calendar_temp')\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        *\n",
        "        ,DAYOFWEEK(date) AS DayOfWeekNum\n",
        "        ,CASE WHEN ( YEAR(date) = YEAR(CURRENT_DATE()) ) THEN 1 ELSE 0 END  AS IsCurrentYear\n",
        "        ,CASE WHEN ( YEAR(date) = YEAR(CURRENT_DATE())-1 ) THEN 1 ELSE 0 END  AS IsPreviousYear\n",
        "        ,CASE WHEN ( YEAR(date) = YEAR(CURRENT_DATE()) AND QUARTER(date) = QUARTER(CURRENT_DATE()) ) THEN 1 ELSE 0 END  AS IsCurrentQuarter\n",
        "        ,CASE WHEN ( YEAR(date) = YEAR(CURRENT_DATE()) ) THEN 1 ELSE 0 END  AS IsCurrentMonth\n",
        "        ,CASE WHEN ( DATE_FORMAT(date, 'yyyy-MM') = DATE_FORMAT(ADD_MONTHS(CURRENT_DATE(), -1), 'yyyy-MM') ) THEN 1 ELSE 0 END  AS IsPreviousMonth\n",
        "        ,CASE WHEN ( date BETWEEN DATE_ADD(CURRENT_DATE(), -14) AND CURRENT_DATE() ) THEN 1 ELSE 0 END  AS IsInLast14Days\n",
        "        ,CASE WHEN ( date BETWEEN DATE_ADD(CURRENT_DATE(), -30) AND CURRENT_DATE() ) THEN 1 ELSE 0 END  AS IsInLast30Days\n",
        "    FROM calendar_temp\n",
        "\"\"\"\n",
        "\n",
        "final_date_df = spark.sql(query)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "03d5f01b-7423-425c-9273-326b8dc632ea"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    display(final_date_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "bf990879-f4d9-432b-8f66-77280dd399a7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the DataFrame to the lakehouse\n",
        "final_date_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"calendar\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "bcbf871a-ef5d-4252-87d4-ca031e17a2e0"
    }
  ],
  "metadata": {
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "sessionKeepAliveTimeout": 0,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "a365ComputeOptions": null,
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d"
      },
      "environment": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}