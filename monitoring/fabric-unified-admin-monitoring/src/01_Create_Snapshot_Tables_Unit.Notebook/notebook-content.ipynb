{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Snapshot Tables\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #88D5FF;\">**Delete, if day exists, then Append**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "****\n",
        "\n",
        "##### Source:\n",
        "\n",
        "** Selected Tables from Lakehouse** in FUAM_Lakehouse\n",
        "\n",
        "##### Target:\n",
        "** History Tables from Lakehouse** in FUAM_Lakehouse\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {},
      "id": "e6f3fbdd-e0f2-48ce-82f6-a393d512149e"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pyspark.sql.functions import col, lit, udf, explode, to_date, json_tuple, from_json, schema_of_json, get_json_object, concat\n",
        "from pyspark.sql.types import StringType, json\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "from delta.tables import *\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import *\n",
        "import datetime"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "f03f0365-c0b9-433b-a0d1-17bd3c7966bf"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "b5e7d90a-f8d8-4391-8ad3-09b543c19024"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "tables = [\n",
        "    {'name' :'active_items' , 'snapshot_id_cols' : ['id', 'capacityId', 'workspaceId']},\n",
        "    {'name' :'capacities'  , 'snapshot_id_cols' : ['CapacityId']},\n",
        "    {'name' :'workspaces'  , 'snapshot_id_cols' : ['CapacityId', 'WorkspaceId']},\n",
        "    {'name' :'workspaces_scanned_users'  , 'snapshot_id_cols' : ['WorkspaceId']}\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "c69faf2b-0bf5-4b43-8fc9-bb44d210f886"
    },
    {
      "cell_type": "code",
      "source": [
        "for table in tables:\n",
        "\n",
        "    table_name = table['name']\n",
        "    ids = table['snapshot_id_cols']\n",
        "    history_table = table_name + '_history'\n",
        "\n",
        "    print(history_table)\n",
        "    curr_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    df = spark.sql(f\"SELECT * FROM {table_name}\" )\n",
        "    df = df.withColumn(\"Snapshot_Date\", to_date(lit(curr_date)))\n",
        "    for id in ids:\n",
        "        df = df.withColumn(\"Snapshot_\" + id , concat(lit(curr_date),lit(\"_\"), col(id))) \n",
        "\n",
        "    if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', history_table ):\n",
        "\n",
        "        print(\"table exists\")\n",
        "        sql = \"\"\" DELETE FROM \"\"\" + history_table + \"\"\"\n",
        "        WHERE Snapshot_Date = '\"\"\" + curr_date + \"\"\"'  \n",
        "        \"\"\"\n",
        "\n",
        "        spark.sql(sql)\n",
        "        df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(history_table)\n",
        "\n",
        "    else:\n",
        "        print(\"History table will be created.\")\n",
        "\n",
        "        df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(history_table)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false
      },
      "id": "889055ee-f007-4f01-9787-19b9a0ab7725"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "known_lakehouses": [
          {
            "id": "6cff634b-88f7-3505-bed2-c03a36776a8b"
          }
        ],
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d"
      },
      "environment": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}