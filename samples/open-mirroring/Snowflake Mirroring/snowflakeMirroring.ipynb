{"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","!!pip install snowflake-connector-python"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"c3063245-6ce1-4245-bd35-5954747402f0","normalized_state":"finished","queued_time":"2025-07-20T18:49:03.0287636Z","session_start_time":null,"execution_start_time":"2025-07-20T18:49:03.0299958Z","execution_finish_time":"2025-07-20T18:49:06.3567226Z","parent_msg_id":"ced21920-42a1-4ee3-ab79-ad972aecc9be"},"text/plain":"StatementMeta(, c3063245-6ce1-4245-bd35-5954747402f0, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"['Requirement already satisfied: snowflake-connector-python in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (3.16.0)',\n 'Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (1.5.1)',\n 'Requirement already satisfied: boto3>=1.24 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (1.39.9)',\n 'Requirement already satisfied: botocore>=1.24 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (1.39.9)',\n 'Requirement already satisfied: cffi<2.0.0,>=1.9 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (1.16.0)',\n 'Requirement already satisfied: cryptography>=3.1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (42.0.2)',\n 'Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (24.0.0)',\n 'Requirement already satisfied: pyjwt<3.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)',\n 'Requirement already satisfied: pytz in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3.post1)',\n 'Requirement already satisfied: requests<3.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)',\n 'Requirement already satisfied: packaging in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (23.1)',\n 'Requirement already satisfied: charset_normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (2.0.4)',\n 'Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)',\n 'Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (2024.2.2)',\n 'Requirement already satisfied: typing_extensions<5,>=4.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (4.9.0)',\n 'Requirement already satisfied: filelock<4,>=3.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (3.13.1)',\n 'Requirement already satisfied: sortedcontainers>=2.4.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)',\n 'Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (3.10.0)',\n 'Requirement already satisfied: tomlkit in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from snowflake-connector-python) (0.13.3)',\n 'Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)',\n 'Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (0.13.1)',\n 'Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.8.2)',\n 'Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.1.0)',\n 'Requirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)',\n 'Requirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)']"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb15ac3a-534a-438b-8801-55f1003fc990"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"52f1eeac-7ded-4569-b386-7922735d47dd"},{"cell_type":"markdown","source":["# Setup stream for the Table / View / Dynamic table\n","\n","This code creates the stream that stores the changes/deltas."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4d277c7-5332-4f49-9e03-bdab934b80e0"},{"cell_type":"code","source":["import pandas as pd\n","import snowflake.connector\n","\n","# Your Snowflake credentials\n","SF_CONFIG = {\n","    \"user\": \"Snowflake_User\",\n","    \"password\": \"Snowflake_password\",\n","    \"account\": \"abcd-xy12345.east-us-2.azure\",  # e.g. abcd-xy12345.east-us-2.azure\n","    \"warehouse\": \"Snowflake_warehouse\",\n","    \"database\": \"Snowflake_Database\",\n","    \"schema\": \"PUBLIC\"\n","}\n","\n","\n","def get_object_type(conn, database, schema, object_name):\n","    \"\"\"Returns the object type: TABLE, VIEW, MATERIALIZED VIEW, or DYNAMIC TABLE\"\"\"\n","    query = f\"\"\"\n","        SELECT  \n","        case \n","        when TABLE_TYPE = 'VIEW' then 'VIEW' \n","        when TABLE_TYPE = 'BASE TABLE' and IS_DYNAMIC = 'YES' then 'DYNAMIC TABLE' \n","        else 'TABLE' end as TABLE_TYPE  FROM INFORMATION_SCHEMA.TABLES\n","        WHERE TABLE_SCHEMA = '{schema}'\n","          AND TABLE_NAME = '{object_name.upper()}'\n","    \"\"\"\n","    with conn.cursor() as cur:\n","        cur.execute(query)\n","        result = cur.fetchone()\n","        return result[0] if result else None\n","\n","\n","def create_stream_if_supported(conn, database, schema, object_name, stream_name):\n","    \"\"\"Create a stream if the object is a table or dynamic table\"\"\"\n","    object_type = get_object_type(conn, database, schema, object_name)\n","    if object_type in ('VIEW','TABLE', 'DYNAMIC TABLE'):\n","        fully_qualified_object = f'{schema}.{object_name}'\n","        fully_qualified_stream = f'{schema}.{stream_name}'\n","        stream_query = f\"\"\"\n","                CREATE OR REPLACE STREAM {fully_qualified_stream}\n","                ON {object_type} {fully_qualified_object};\n","            \"\"\"\n","        display(stream_query)\n","        with conn.cursor() as cur:\n","            cur.execute(stream_query)\n","        print(f\"‚úÖ Created stream `{stream_name}` on `{object_name}` ({object_type})\")\n","    else:\n","        print(f\"‚ö†Ô∏è Skipped `{object_name}` ({object_type}) ‚Äî streams not supported.\")\n","\n","\n","def fetch_table_snapshot(conn, object_name, output_path):\n","    \"\"\"Fetch full snapshot of a table/dynamic table and write to Parquet.\"\"\"\n","    df = pd.read_sql(f'SELECT * FROM {object_name}', conn)\n","    df.to_parquet(output_path, index=False)\n","    print(f\"üì¶ Snapshot written to {output_path}\")\n","    return df\n","\n","\n","def fetch_stream_changes(conn, stream_name, output_path):\n","    \"\"\"Fetch changes from stream and write to Parquet.\"\"\"\n","    query = f\"\"\"\n","        SELECT *,\n","            CASE METADATA$ACTION\n","                WHEN 'INSERT' THEN 1\n","                WHEN 'DELETE' THEN 2\n","                WHEN 'UPDATE' THEN 1\n","                ELSE 1\n","            END AS __rowMarker__\n","        FROM {stream_name} order by metadata$row_id, METADATA$action;\n","    \"\"\"\n","    df = pd.read_sql(query, conn)\n","    df.to_parquet(output_path, index=False)\n","    print(f\"üì¶ Stream changes written to {output_path}\")\n","    return df\n","\n","my_client_id=\"<Service Principal Id>\"\n","my_client_secret=\"<Secret>\"\n","my_client_tenant=\"<Azure Tenant Id>\"\n","\n","landing_zone = \"https://onelake.dfs.fabric.microsoft.com/<Workspace Id>/<Mirrored Database Id>/Files/LandingZone\"\n","\n","# Setup the OpenMirroring Client\n","client = OpenMirroringClient(\n","    client_id=my_client_id,\n","    client_secret=my_client_secret,\n","    client_tenant=my_client_tenant,\n","    host=landing_zone\n",")\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":80,"statement_ids":[80],"state":"finished","livy_statement_state":"available","session_id":"c3063245-6ce1-4245-bd35-5954747402f0","normalized_state":"finished","queued_time":"2025-07-20T20:22:54.5138857Z","session_start_time":null,"execution_start_time":"2025-07-20T20:22:54.5150905Z","execution_finish_time":"2025-07-20T20:22:54.7814842Z","parent_msg_id":"dc4f31b5-8329-473c-8eb2-c9d94df8f1c4"},"text/plain":"StatementMeta(, c3063245-6ce1-4245-bd35-5954747402f0, 80, Finished, Available, Finished)"},"metadata":{}}],"execution_count":78,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fb50021-c51e-4048-8e7c-f56bceaba726"},{"cell_type":"code","source":["# ‚ùó Main logic ‚Äî loop over multiple objects\n","if __name__ == \"__main__\":\n","    conn = snowflake.connector.connect(**SF_CONFIG)\n","    db = SF_CONFIG[\"database\"]\n","    schema = SF_CONFIG[\"schema\"]\n","    file_location =\"/lakehouse/default/Files/\"\n","\n","    # These are the tables, views and dynamic tables that will be mirrored\n","    object_names = ['CUSTOMER_SMALL',  'view_customer_small','dynamic_table_customer_demo']  # List of table/view names\n","\n","    for obj in object_names:\n","        stream_name = f\"{obj}_STREAM\"\n","        snapshot_file = f\"{file_location}{obj}_snapshot.parquet\"\n","        client.remove_table(schema_name=schema , table_name=obj)\n","        client.create_table(schema_name=schema , table_name=obj, key_cols=[\"C_CUSTKEY\"])\n","        create_stream_if_supported(conn, db, schema, obj, stream_name)\n","        fetch_table_snapshot(conn, f'{schema}.{obj}', snapshot_file)\n","        client.upload_data_file(schema_name=schema , table_name=obj, local_file_path=snapshot_file)\n","    conn.close()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":72,"statement_ids":[72],"state":"finished","livy_statement_state":"available","session_id":"c3063245-6ce1-4245-bd35-5954747402f0","normalized_state":"finished","queued_time":"2025-07-20T20:11:27.4060024Z","session_start_time":null,"execution_start_time":"2025-07-20T20:11:27.4072722Z","execution_finish_time":"2025-07-20T20:11:37.2748833Z","parent_msg_id":"eef1f6d6-a7ac-4e78-9ede-1f970601badb"},"text/plain":"StatementMeta(, c3063245-6ce1-4245-bd35-5954747402f0, 72, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Folder 'PUBLIC.schema/CUSTOMER_SMALL' deleted successfully.\nFolder and _metadata.json created successfully at: PUBLIC.schema/CUSTOMER_SMALL\n"]},{"output_type":"display_data","data":{"text/plain":"'\\n                CREATE OR REPLACE STREAM PUBLIC.CUSTOMER_SMALL_STREAM\\n                ON TABLE PUBLIC.CUSTOMER_SMALL;\\n            '"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üì¶ Snapshot written to /lakehouse/default/Files/CUSTOMER_SMALL_snapshot.parquet\nFile uploaded successfully as '_00000000000000000001.parquet'.\nFile renamed from _00000000000000000001.parquet to 00000000000000000001.parquet successfully.\nFile renamed successfully to '00000000000000000001.parquet'.\nFolder 'PUBLIC.schema/view_customer_small' deleted successfully.\nFolder and _metadata.json created successfully at: PUBLIC.schema/view_customer_small\n"]},{"output_type":"display_data","data":{"text/plain":"'\\n                CREATE OR REPLACE STREAM PUBLIC.view_customer_small_STREAM\\n                ON VIEW PUBLIC.view_customer_small;\\n            '"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Created stream `view_customer_small_STREAM` on `view_customer_small` (VIEW)\nüì¶ Snapshot written to /lakehouse/default/Files/view_customer_small_snapshot.parquet\nFile uploaded successfully as '_00000000000000000001.parquet'.\nFile renamed from _00000000000000000001.parquet to 00000000000000000001.parquet successfully.\nFile renamed successfully to '00000000000000000001.parquet'.\nFolder 'PUBLIC.schema/dynamic_table_customer_demo' deleted successfully.\nFolder and _metadata.json created successfully at: PUBLIC.schema/dynamic_table_customer_demo\n"]},{"output_type":"display_data","data":{"text/plain":"'\\n                CREATE OR REPLACE STREAM PUBLIC.dynamic_table_customer_demo_STREAM\\n                ON DYNAMIC TABLE PUBLIC.dynamic_table_customer_demo;\\n            '"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Created stream `dynamic_table_customer_demo_STREAM` on `dynamic_table_customer_demo` (DYNAMIC TABLE)\nüì¶ Snapshot written to /lakehouse/default/Files/dynamic_table_customer_demo_snapshot.parquet\nFile uploaded successfully as '_00000000000000000001.parquet'.\nFile renamed from _00000000000000000001.parquet to 00000000000000000001.parquet successfully.\nFile renamed successfully to '00000000000000000001.parquet'.\n"]}],"execution_count":70,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f45ab01b-fbf5-4d6a-962b-a97f60b48395"},{"cell_type":"code","source":["# ‚ùó Main logic ‚Äî loop over multiple objects\n","# For the CDC, I drop and recreate the stream after I collect the changes.\n","# this keeps the streams small\n","if __name__ == \"__main__\":\n","    conn = snowflake.connector.connect(**SF_CONFIG)\n","    db = SF_CONFIG[\"database\"]\n","    schema = SF_CONFIG[\"schema\"]\n","    file_location =\"/lakehouse/default/Files/\"\n","\n","    for obj in object_names:\n","        stream_name = f\"{obj}_STREAM\"\n","        snapshot_file = f\"{file_location}{obj}_changes.parquet\"\n","        fetch_stream_changes(conn, f'{schema}.{stream_name}', snapshot_file)\n","        create_stream_if_supported(conn, db, schema, obj, stream_name)\n","        client.upload_data_file(schema_name=schema , table_name=obj, local_file_path=snapshot_file)\n","\n","    conn.close()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":83,"statement_ids":[83],"state":"finished","livy_statement_state":"available","session_id":"c3063245-6ce1-4245-bd35-5954747402f0","normalized_state":"finished","queued_time":"2025-07-20T20:27:47.2759647Z","session_start_time":null,"execution_start_time":"2025-07-20T20:27:47.2774585Z","execution_finish_time":"2025-07-20T20:27:55.770407Z","parent_msg_id":"b638e8e3-c7c2-47a5-9664-cbcf9024a1de"},"text/plain":"StatementMeta(, c3063245-6ce1-4245-bd35-5954747402f0, 83, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipykernel_6068/281612742.py:70: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  df = pd.read_sql(query, conn)\n"]},{"output_type":"stream","name":"stdout","text":["üì¶ Stream changes written to /lakehouse/default/Files/CUSTOMER_SMALL_changes.parquet\n"]},{"output_type":"display_data","data":{"text/plain":"'\\n                CREATE OR REPLACE STREAM PUBLIC.CUSTOMER_SMALL_STREAM\\n                ON TABLE PUBLIC.CUSTOMER_SMALL;\\n            '"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Created stream `CUSTOMER_SMALL_STREAM` on `CUSTOMER_SMALL` (TABLE)\nFile uploaded successfully as '_00000000000000000007.parquet'.\nFile renamed from _00000000000000000007.parquet to 00000000000000000007.parquet successfully.\nFile renamed successfully to '00000000000000000007.parquet'.\nüì¶ Stream changes written to /lakehouse/default/Files/view_customer_small_changes.parquet\n"]},{"output_type":"display_data","data":{"text/plain":"'\\n                CREATE OR REPLACE STREAM PUBLIC.view_customer_small_STREAM\\n                ON VIEW PUBLIC.view_customer_small;\\n            '"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Created stream `view_customer_small_STREAM` on `view_customer_small` (VIEW)\nFile uploaded successfully as '_00000000000000000007.parquet'.\nFile renamed from _00000000000000000007.parquet to 00000000000000000007.parquet successfully.\nFile renamed successfully to '00000000000000000007.parquet'.\nüì¶ Stream changes written to /lakehouse/default/Files/dynamic_table_customer_demo_changes.parquet\n"]},{"output_type":"display_data","data":{"text/plain":"'\\n                CREATE OR REPLACE STREAM PUBLIC.dynamic_table_customer_demo_STREAM\\n                ON DYNAMIC TABLE PUBLIC.dynamic_table_customer_demo;\\n            '"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Created stream `dynamic_table_customer_demo_STREAM` on `dynamic_table_customer_demo` (DYNAMIC TABLE)\nFile uploaded successfully as '_00000000000000000007.parquet'.\nFile renamed from _00000000000000000007.parquet to 00000000000000000007.parquet successfully.\nFile renamed successfully to '00000000000000000007.parquet'.\n"]}],"execution_count":81,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"00d016cb-e512-4d26-9e05-494d94f6ac4c"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n","# Licensed under the MIT License.\n","\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import ClientSecretCredential\n","import requests\n","import json\n","import os\n","\n","class OpenMirroringClient:\n","    def __init__(self, client_id: str, client_secret: str, client_tenant: str, host: str):\n","        self.client_id = client_id\n","        self.client_secret = client_secret\n","        self.client_tenant = client_tenant\n","        self.host = self._normalize_path(host)\n","        self.service_client = self._create_service_client()\n","\n","    def _normalize_path(self, path: str) -> str:\n","        \"\"\"\n","        Normalizes the given path by removing the 'LandingZone' segment if it ends with it.\n","\n","        :param path: The original path.\n","        :return: The normalized path.\n","        \"\"\"\n","        if path.endswith(\"LandingZone\"):\n","            # Remove the 'LandingZone' segment\n","            return path[:path.rfind(\"/LandingZone\")]\n","        elif path.endswith(\"LandingZone/\"):\n","            # Remove the 'LandingZone/' segment\n","            return path[:path.rfind(\"/LandingZone/\")]\n","        return path\n","\n","    def _create_service_client(self):\n","        \"\"\"Creates and returns a DataLakeServiceClient.\"\"\"\n","        try:\n","            credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","            return DataLakeServiceClient(account_url=self.host, credential=credential)\n","        except Exception as e:\n","            raise Exception(f\"Failed to create DataLakeServiceClient: {e}\")\n","\n","    def create_table(self, schema_name: str = None, table_name: str = \"\", key_cols: list = []):\n","        \"\"\"\n","        Creates a folder in OneLake storage and a _metadata.json file inside it.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param key_cols: List of key column names.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Create the folder\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","            directory_client.create_directory()\n","\n","            # Create the _metadata.json file\n","            metadata_content = {\"keyColumns\": [f'{col}' for col in key_cols]}\n","            metadata_file_path = os.path.join(folder_path, \"_metadata.json\")\n","            file_client = directory_client.create_file(\"_metadata.json\")\n","            file_client.append_data(data=json.dumps(metadata_content), offset=0, length=len(json.dumps(metadata_content)))\n","            file_client.flush_data(len(json.dumps(metadata_content)))\n","\n","            print(f\"Folder and _metadata.json created successfully at: {folder_path}\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to create table: {e}\")\n","\n","    def remove_table(self, schema_name: str = None, table_name: str = \"\", remove_schema_folder: bool = False):\n","        \"\"\"\n","        Deletes a folder in the OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param remove_schema_folder: If True, removes the schema folder as well.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                print(f\"Warning: Folder '{folder_path}' not found.\")\n","                return\n","\n","            # Delete the folder\n","            directory_client.delete_directory()\n","            print(f\"Folder '{folder_path}' deleted successfully.\")\n","\n","            # Check if schema folder exists\n","            if remove_schema_folder and schema_name:\n","                schema_folder_path = f\"{schema_name}.schema\"\n","                schema_directory_client = file_system_client.get_directory_client(schema_folder_path)\n","                if schema_directory_client.exists():\n","                    schema_directory_client.delete_directory()\n","                    print(f\"Schema folder '{schema_folder_path}' deleted successfully.\")\n","                else:\n","                    print(f\"Warning: Schema folder '{schema_folder_path}' not found.\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to delete table: {e}\")\n","\n","    def get_next_file_name(self, schema_name: str = None, table_name: str = \"\") -> str:\n","        \"\"\"\n","        Finds the next file name for a folder in OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :return: The next file name padded to 20 digits.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"LandingZone/{schema_name}.schema/{table_name}\" if schema_name else f\"LandingZone/{table_name}\"\n","\n","        try:\n","            # Get the system client\n","            file_system_client = self.service_client.get_file_system_client(file_system=folder_path)\n","\n","            # List all files in the folder\n","            file_list = file_system_client.get_paths(recursive=False)\n","            parquet_files = []\n","\n","            for file in file_list:\n","                file_name = os.path.basename(file.name)\n","                if not file.is_directory and file_name.endswith(\".parquet\") and not file_name.startswith(\"_\"):\n","                    # Validate the file name pattern\n","                    if not file_name[:-8].isdigit() or len(file_name[:-8]) != 20:  # Exclude \".parquet\"\n","                        raise ValueError(f\"Invalid file name pattern: {file_name}\")\n","                    parquet_files.append(int(file_name[:-8]))\n","\n","            # Determine the next file name\n","            if parquet_files:\n","                next_file_number = max(parquet_files) + 1\n","            else:\n","                next_file_number = 1\n","\n","            # Return the next file name padded to 20 digits\n","            return f\"{next_file_number:020}.parquet\"\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to get next file name: {e}\")\n","\n","    def upload_data_file(self, schema_name: str = None, table_name: str = \"\", local_file_path: str = \"\"):\n","        \"\"\"\n","        Uploads a file to OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param local_file_path: Path to the local file to be uploaded.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","        if not local_file_path or not os.path.isfile(local_file_path):\n","            raise ValueError(\"Invalid local file path.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                raise FileNotFoundError(f\"Folder '{folder_path}' not found.\")\n","\n","            # Get the next file name\n","            next_file_name = self.get_next_file_name(schema_name, table_name)\n","\n","            # Add an underscore to the file name for temporary upload\n","            temp_file_name = f\"_{next_file_name}\"\n","\n","            # Upload the file\n","            file_client = directory_client.create_file(temp_file_name)\n","            with open(local_file_path, \"rb\") as file_data:\n","                file_contents = file_data.read()\n","                file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n","                file_client.flush_data(len(file_contents))\n","\n","            print(f\"File uploaded successfully as '{temp_file_name}'.\")\n","            \n","            # Python SDK doesn't handle rename properly for onelake, using REST API to rename the file instead\n","            self.rename_file_via_rest_api(f\"LandingZone/{folder_path}\", temp_file_name, next_file_name)\n","            print(f\"File renamed successfully to '{next_file_name}'.\")\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to upload data file: {e}\")\n","        \n","    def rename_file_via_rest_api(self, folder_path: str, old_file_name: str, new_file_name: str):\n","        # Create a ClientSecretCredential\n","        credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","        # Get a token\n","        token = credential.get_token(\"https://storage.azure.com/.default\").token\n","\n","        # Construct the rename URL\n","        rename_url = f\"{self.host}/{folder_path}/{new_file_name}\"\n","\n","        # Construct the source path\n","        source_path = f\"{self.host}/{folder_path}/{old_file_name}\"\n","\n","        # Set the headers\n","        headers = {\n","            \"Authorization\": f\"Bearer {token}\",\n","            \"x-ms-rename-source\": source_path,\n","            \"x-ms-version\": \"2020-06-12\"\n","        }\n","\n","        # Send the rename request\n","        response = requests.put(rename_url, headers=headers)\n","\n","        if response.status_code in [200, 201]:\n","            print(f\"File renamed from {old_file_name} to {new_file_name} successfully.\")\n","        else:\n","            print(f\"Failed to rename file. Status code: {response.status_code}, Error: {response.text}\")\n","\n","    def get_mirrored_database_status(self):\n","        \"\"\"\n","        Retrieves and displays the status of the mirrored database from Monitoring/replicator.json.\n","\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"replicator.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","            print(json.dumps(status_json, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","    def get_table_status(self, schema_name: str = None, table_name: str = None):\n","        \"\"\"\n","        Retrieves and displays the status of tables from Monitoring/table.json.\n","\n","        :param schema_name: Optional schema name to filter.\n","        :param table_name: Optional table name to filter.\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"tables.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","\n","            # Treat None as empty string for filtering\n","            schema_name = schema_name or \"\"\n","            table_name = table_name or \"\"\n","\n","            if not schema_name and not table_name:\n","                # Show the whole JSON content\n","                print(json.dumps(status_json, indent=4))\n","            else:\n","                # Filter tables array\n","                filtered_tables = [\n","                    t for t in status_json.get(\"tables\", [])\n","                    if t.get(\"sourceSchemaName\", \"\") == schema_name and t.get(\"sourceTableName\", \"\") == table_name\n","                ]\n","                print(json.dumps({\"tables\": filtered_tables}, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":27,"statement_ids":[27],"state":"finished","livy_statement_state":"available","session_id":"c3063245-6ce1-4245-bd35-5954747402f0","normalized_state":"finished","queued_time":"2025-07-20T19:28:08.7989115Z","session_start_time":null,"execution_start_time":"2025-07-20T19:28:08.8007265Z","execution_finish_time":"2025-07-20T19:28:09.0844397Z","parent_msg_id":"9b7f3b95-24b1-4ac4-b2c5-c7dc45e01024"},"text/plain":"StatementMeta(, c3063245-6ce1-4245-bd35-5954747402f0, 27, Finished, Available, Finished)"},"metadata":{}}],"execution_count":25,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8af797fd-5de5-4ec6-b0a1-389013f21fb7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"version":"0.1","state":{}},"dependencies":{"lakehouse":{"default_lakehouse":"0ef2c045-1247-44ad-b722-fe2b46525e02","known_lakehouses":[{"id":"0ef2c045-1247-44ad-b722-fe2b46525e02"}],"default_lakehouse_name":"lakeexcel","default_lakehouse_workspace_id":"3e610b5b-7da7-42fa-b78f-faef8b8affe4"}}},"nbformat":4,"nbformat_minor":5}