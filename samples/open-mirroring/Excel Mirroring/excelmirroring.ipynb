{"cells":[{"cell_type":"markdown","source":["# Excel Mirroring Example\n","\n","## Overview\n","\n","Simple notebook example for Open Mirroring, this example scans a folder in Onelake, and pushing the data into Fabric using Open Mirroring.\n","\n","## Process\n","1. Scan the folder in Onelake for Excel files\n","1. Open each Excel file up and save each workbook as a seperate file.\n","1. Each Excel document is turned into a schema and each workbook becomes a table\n","1. Use the OpenMirroringPythonSDK to create a mirrored table and upload the file to the Mirrored database.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4818bd3b-a5ef-4e6e-a996-24792861a9ac"},{"cell_type":"markdown","source":["This block below is just a copy of the code from here;   https://github.com/microsoft/fabric-toolbox/tree/main/tools/OpenMirroringPythonSDK"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a08585f3-c688-4010-bf45-90fd32cbb7c1"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n","# Licensed under the MIT License.\n","\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import ClientSecretCredential\n","import requests\n","import json\n","import os\n","\n","class OpenMirroringClient:\n","    def __init__(self, client_id: str, client_secret: str, client_tenant: str, host: str):\n","        self.client_id = client_id\n","        self.client_secret = client_secret\n","        self.client_tenant = client_tenant\n","        self.host = self._normalize_path(host)\n","        self.service_client = self._create_service_client()\n","\n","    def _normalize_path(self, path: str) -> str:\n","        \"\"\"\n","        Normalizes the given path by removing the 'LandingZone' segment if it ends with it.\n","\n","        :param path: The original path.\n","        :return: The normalized path.\n","        \"\"\"\n","        if path.endswith(\"LandingZone\"):\n","            # Remove the 'LandingZone' segment\n","            return path[:path.rfind(\"/LandingZone\")]\n","        elif path.endswith(\"LandingZone/\"):\n","            # Remove the 'LandingZone/' segment\n","            return path[:path.rfind(\"/LandingZone/\")]\n","        return path\n","\n","    def _create_service_client(self):\n","        \"\"\"Creates and returns a DataLakeServiceClient.\"\"\"\n","        try:\n","            credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","            return DataLakeServiceClient(account_url=self.host, credential=credential)\n","        except Exception as e:\n","            raise Exception(f\"Failed to create DataLakeServiceClient: {e}\")\n","\n","    def create_table(self, schema_name: str = None, table_name: str = \"\", key_cols: list = []):\n","        \"\"\"\n","        Creates a folder in OneLake storage and a _metadata.json file inside it.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param key_cols: List of key column names.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Create the folder\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","            directory_client.create_directory()\n","\n","            # Create the _metadata.json file\n","            metadata_content = {\"keyColumns\": [f'{col}' for col in key_cols]}\n","            metadata_file_path = os.path.join(folder_path, \"_metadata.json\")\n","            file_client = directory_client.create_file(\"_metadata.json\")\n","            file_client.append_data(data=json.dumps(metadata_content), offset=0, length=len(json.dumps(metadata_content)))\n","            file_client.flush_data(len(json.dumps(metadata_content)))\n","\n","            print(f\"Folder and _metadata.json created successfully at: {folder_path}\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to create table: {e}\")\n","\n","    def remove_table(self, schema_name: str = None, table_name: str = \"\", remove_schema_folder: bool = False):\n","        \"\"\"\n","        Deletes a folder in the OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param remove_schema_folder: If True, removes the schema folder as well.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                print(f\"Warning: Folder '{folder_path}' not found.\")\n","                return\n","\n","            # Delete the folder\n","            directory_client.delete_directory()\n","            print(f\"Folder '{folder_path}' deleted successfully.\")\n","\n","            # Check if schema folder exists\n","            if remove_schema_folder and schema_name:\n","                schema_folder_path = f\"{schema_name}.schema\"\n","                schema_directory_client = file_system_client.get_directory_client(schema_folder_path)\n","                if schema_directory_client.exists():\n","                    schema_directory_client.delete_directory()\n","                    print(f\"Schema folder '{schema_folder_path}' deleted successfully.\")\n","                else:\n","                    print(f\"Warning: Schema folder '{schema_folder_path}' not found.\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to delete table: {e}\")\n","\n","    def get_next_file_name(self, schema_name: str = None, table_name: str = \"\") -> str:\n","        \"\"\"\n","        Finds the next file name for a folder in OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :return: The next file name padded to 20 digits.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"LandingZone/{schema_name}.schema/{table_name}\" if schema_name else f\"LandingZone/{table_name}\"\n","\n","        try:\n","            # Get the system client\n","            file_system_client = self.service_client.get_file_system_client(file_system=folder_path)\n","\n","            # List all files in the folder\n","            file_list = file_system_client.get_paths(recursive=False)\n","            parquet_files = []\n","\n","            for file in file_list:\n","                file_name = os.path.basename(file.name)\n","                if not file.is_directory and file_name.endswith(\".parquet\") and not file_name.startswith(\"_\"):\n","                    # Validate the file name pattern\n","                    if not file_name[:-8].isdigit() or len(file_name[:-8]) != 20:  # Exclude \".parquet\"\n","                        raise ValueError(f\"Invalid file name pattern: {file_name}\")\n","                    parquet_files.append(int(file_name[:-8]))\n","\n","            # Determine the next file name\n","            if parquet_files:\n","                next_file_number = max(parquet_files) + 1\n","            else:\n","                next_file_number = 1\n","\n","            # Return the next file name padded to 20 digits\n","            return f\"{next_file_number:020}.parquet\"\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to get next file name: {e}\")\n","\n","    def upload_data_file(self, schema_name: str = None, table_name: str = \"\", local_file_path: str = \"\"):\n","        \"\"\"\n","        Uploads a file to OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param local_file_path: Path to the local file to be uploaded.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","        if not local_file_path or not os.path.isfile(local_file_path):\n","            raise ValueError(\"Invalid local file path.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                raise FileNotFoundError(f\"Folder '{folder_path}' not found.\")\n","\n","            # Get the next file name\n","            next_file_name = self.get_next_file_name(schema_name, table_name)\n","\n","            # Add an underscore to the file name for temporary upload\n","            temp_file_name = f\"_{next_file_name}\"\n","\n","            # Upload the file\n","            file_client = directory_client.create_file(temp_file_name)\n","            with open(local_file_path, \"rb\") as file_data:\n","                file_contents = file_data.read()\n","                file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n","                file_client.flush_data(len(file_contents))\n","\n","            print(f\"File uploaded successfully as '{temp_file_name}'.\")\n","            \n","            # Python SDK doesn't handle rename properly for onelake, using REST API to rename the file instead\n","            self.rename_file_via_rest_api(f\"LandingZone/{folder_path}\", temp_file_name, next_file_name)\n","            print(f\"File renamed successfully to '{next_file_name}'.\")\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to upload data file: {e}\")\n","        \n","    def rename_file_via_rest_api(self, folder_path: str, old_file_name: str, new_file_name: str):\n","        # Create a ClientSecretCredential\n","        credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","        # Get a token\n","        token = credential.get_token(\"https://storage.azure.com/.default\").token\n","\n","        # Construct the rename URL\n","        rename_url = f\"{self.host}/{folder_path}/{new_file_name}\"\n","\n","        # Construct the source path\n","        source_path = f\"{self.host}/{folder_path}/{old_file_name}\"\n","\n","        # Set the headers\n","        headers = {\n","            \"Authorization\": f\"Bearer {token}\",\n","            \"x-ms-rename-source\": source_path,\n","            \"x-ms-version\": \"2020-06-12\"\n","        }\n","\n","        # Send the rename request\n","        response = requests.put(rename_url, headers=headers)\n","\n","        if response.status_code in [200, 201]:\n","            print(f\"File renamed from {old_file_name} to {new_file_name} successfully.\")\n","        else:\n","            print(f\"Failed to rename file. Status code: {response.status_code}, Error: {response.text}\")\n","\n","    def get_mirrored_database_status(self):\n","        \"\"\"\n","        Retrieves and displays the status of the mirrored database from Monitoring/replicator.json.\n","\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"replicator.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","            print(json.dumps(status_json, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","    def get_table_status(self, schema_name: str = None, table_name: str = None):\n","        \"\"\"\n","        Retrieves and displays the status of tables from Monitoring/table.json.\n","\n","        :param schema_name: Optional schema name to filter.\n","        :param table_name: Optional table name to filter.\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"tables.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","\n","            # Treat None as empty string for filtering\n","            schema_name = schema_name or \"\"\n","            table_name = table_name or \"\"\n","\n","            if not schema_name and not table_name:\n","                # Show the whole JSON content\n","                print(json.dumps(status_json, indent=4))\n","            else:\n","                # Filter tables array\n","                filtered_tables = [\n","                    t for t in status_json.get(\"tables\", [])\n","                    if t.get(\"sourceSchemaName\", \"\") == schema_name and t.get(\"sourceTableName\", \"\") == table_name\n","                ]\n","                print(json.dumps({\"tables\": filtered_tables}, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"412200cb-79a5-44df-acf6-1ec8e2a7dd20","normalized_state":"finished","queued_time":"2025-07-17T11:20:12.0432257Z","session_start_time":"2025-07-17T11:20:12.0449455Z","execution_start_time":"2025-07-17T11:23:41.3058807Z","execution_finish_time":"2025-07-17T11:23:46.283105Z","parent_msg_id":"1a9d0a77-579a-4789-a3df-ba04cb40a464"},"text/plain":"StatementMeta(, 412200cb-79a5-44df-acf6-1ec8e2a7dd20, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"716c82a4-b0f6-42bf-81e4-802772284dbc"},{"cell_type":"markdown","source":["The below cell, just has some variables needed for mirroring\n","\n","|Variable|Usage|\n","|--------|-----|\n","|excel_path|Folder on Onelake, where the Excel files are.|\n","|temp_file|Location for temp files.|\n","|my_client_id|Service Principal ID|\n","|my_client_secret|The secret|\n","|my_client_tenant|The Azure tenant for the Service Principal|\n","|landing_zone|The Open Mirrroring Landing zone|\n","\n","In the ideal world, put these variables in Key vault.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64f2da50-9552-4dfe-b6df-cd4d285e5f37"},{"cell_type":"code","source":["excel_path = \"/lakehouse/default/Files/excelfiles/\"  # Replace with your Excel file name\n","temp_file = \"/lakehouse/default/Files/temp/\"      # Replace with your desired temp location file name\n","\n","my_client_id=\"<Service Principal Id>\"\n","my_client_secret=\"<Secret>\"\n","my_client_tenant=\"<Azure Tenant Id>\"\n","\n","landing_zone = \"https://onelake.dfs.fabric.microsoft.com/<Workspace Id>/<Mirrored Database Id>/Files/LandingZone\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"412200cb-79a5-44df-acf6-1ec8e2a7dd20","normalized_state":"finished","queued_time":"2025-07-17T11:24:07.2305501Z","session_start_time":null,"execution_start_time":"2025-07-17T11:24:07.2317615Z","execution_finish_time":"2025-07-17T11:24:07.4905047Z","parent_msg_id":"e4609243-300b-424f-82d9-02f7b0c7fa99"},"text/plain":"StatementMeta(, 412200cb-79a5-44df-acf6-1ec8e2a7dd20, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"376bf9d6-e1c6-4399-9d48-bd02c65a8932"},{"cell_type":"markdown","source":["This is the code that extracts the data from the excel file and saves to parquet, then uploads the parquet to the Mirrored Database.\n","\n","(Note: I've seen it a few times, the code that exrtracts from Excel, doesn't extract the latest change, I think this is because the excel document is still open)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5569fd75-9411-4615-88df-c58d6e6c8e7d"},{"cell_type":"code","source":["import pandas as pd\n","import os\n","def Mirror_Excel_File(folder_path,  tmp_location, clean):\n","    try:\n","        # folder_path - where the Excel files are\n","        # tmp_location - temp storage\n","        # clean - if true, we delete the tables from the mirrored database first\n","        # Read the Excel file\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith('.xlsx'):  \n","                excel_file = os.path.join(folder_path, filename)\n","                file_base = os.path.splitext(os.path.basename(excel_file))[0]\n","                xls = pd.ExcelFile(excel_file)\n","                sheet_names = xls.sheet_names\n","                # Convert each sheet to a separate CSV\n","                for sheet in sheet_names:\n","                    if clean == \"true\" :\n","                        client.remove_table(schema_name=file_base, table_name=sheet)\n","                        client.create_table(schema_name=file_base, table_name=sheet, key_cols=[\"__rowid__\"])\n","                    df = pd.read_excel(xls, sheet_name=sheet)\n","                    newpath = f\"{tmp_location}mirroring/{file_base}.schema/\"\n","                    csv_file = f\"{newpath}{sheet}.parquet\"\n","                    df['__rowMarker__'] = '1'\n","                    df['__rowid__'] = range(1, len(df) + 1)\n","                    if not os.path.exists(newpath):\n","                        os.makedirs(newpath)\n","                    if os.path.exists(csv_file):\n","                        os.remove(csv_file)\n","                    df.to_parquet(csv_file, index=False, )\n","                    print(f\"Saved '{sheet}' to '{csv_file}'\")\n","                    client.upload_data_file(schema_name=file_base, table_name=sheet, local_file_path=csv_file)  \n","                    print(f\"Uploaded schema:'{file_base}' to tablename:'{sheet}'\")\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"412200cb-79a5-44df-acf6-1ec8e2a7dd20","normalized_state":"finished","queued_time":"2025-07-17T11:42:22.5012928Z","session_start_time":null,"execution_start_time":"2025-07-17T11:42:22.5024148Z","execution_finish_time":"2025-07-17T11:42:22.76875Z","parent_msg_id":"9e933582-5611-419e-807c-26c9bda55900"},"text/plain":"StatementMeta(, 412200cb-79a5-44df-acf6-1ec8e2a7dd20, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0205c281-2762-4544-bc80-732e8accac2d"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f281c00-6b61-49ec-ba07-d960f2e58c9e"},{"cell_type":"code","source":["# Setup the OpenMirroring Client\n","client = OpenMirroringClient(\n","    client_id=my_client_id,\n","    client_secret=my_client_secret,\n","    client_tenant=my_client_tenant,\n","    host=landing_zone\n",")\n","\n","# Do this for the first time you run it, or you want to reset mirroring\n","Mirror_Excel_File( excel_path,  temp_file ,\"true\")\n","\n","# do this for the 2nd / 3rd runs.\n","Mirror_Excel_File( excel_path,  temp_file ,\"false\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81b94f56-0fa2-4aa8-b94f-d9256c55f097"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"0ef2c045-1247-44ad-b722-fe2b46525e02","known_lakehouses":[{"id":"0ef2c045-1247-44ad-b722-fe2b46525e02"}],"default_lakehouse_name":"lakeexcel","default_lakehouse_workspace_id":"3e610b5b-7da7-42fa-b78f-faef8b8affe4"}}},"nbformat":4,"nbformat_minor":5}