{"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","!pip install mysql-connector-python"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"a898c9c3-8fcd-45f0-a6c2-aabc4c689708","normalized_state":"finished","queued_time":"2025-07-19T15:19:10.5682677Z","session_start_time":"2025-07-19T15:19:10.5691329Z","execution_start_time":"2025-07-19T15:22:41.1158117Z","execution_finish_time":"2025-07-19T15:22:56.3767472Z","parent_msg_id":"2f436b12-35e5-4f16-9625-e1bdb0871caf"},"text/plain":"StatementMeta(, a898c9c3-8fcd-45f0-a6c2-aabc4c689708, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting mysql-connector-python\r\n  Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\r\nDownloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (33.9 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/33.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/33.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/33.9 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/33.9 MB\u001b[0m \u001b[31m282.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/33.9 MB\u001b[0m \u001b[31m289.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m32.5/33.9 MB\u001b[0m \u001b[31m293.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m33.8/33.9 MB\u001b[0m \u001b[31m298.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]},{"output_type":"stream","name":"stdout","text":["Installing collected packages: mysql-connector-python\r\nSuccessfully installed mysql-connector-python-9.3.0\r\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb15ac3a-534a-438b-8801-55f1003fc990"},{"cell_type":"code","source":["import mysql.connector\n","\n","def setup_cdc_for_table(mysqlhost, mysqluser, mysqlpassword, db_name, table_name):\n","    \n","    conn = mysql.connector.connect(\n","    host=mysqlhost,\n","    user=mysqluser,\n","    password=mysqlpassword,\n","    database=db_name\n","    )\n","\n","    cursor = conn.cursor()\n","\n","    # Get column names\n","    cursor.execute(\"\"\"\n","        SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS\n","        WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s\n","        ORDER BY ORDINAL_POSITION\n","    \"\"\", (db_name, table_name))\n","    columns = [row[0] for row in cursor.fetchall()]\n","    \n","    if not columns:\n","        raise Exception(f\"No columns found for {db_name}.{table_name}\")\n","\n","    col_list = \", \".join(f\"`{col}`\" for col in columns)\n","    col_list_new = \", \".join(f\"NEW.`{col}`\" for col in columns)\n","    col_list_old = \", \".join(f\"OLD.`{col}`\" for col in columns)\n","    full_col_list = f\"{col_list}, `__operation_type`, `__changed_at`, `__changed_by`, `__moved`, `__rowMarker__`\"\n","\n","    # Check if _cdc table exists\n","    cdc_table = f\"{table_name}_cdc\"\n","    cursor.execute(\"\"\"\n","        SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n","        WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s\n","    \"\"\", (db_name, cdc_table))\n","    exists = cursor.fetchone()[0]\n","\n","    if  exists:\n","        create_cdc_sql = f\"\"\"\n","        DROP TABLE `{db_name}`.`{cdc_table}`;\n","        \"\"\"\n","        print(\"Drop CDC table:\\n\", create_cdc_sql)\n","        cursor.execute(create_cdc_sql)\n","\n","    create_cdc_sql = f\"\"\"\n","    CREATE TABLE `{db_name}`.`{cdc_table}` AS\n","    SELECT {col_list},\n","            CAST(NULL AS CHAR(20)) AS __operation_type,\n","            CURRENT_TIMESTAMP AS __changed_at,\n","            CURRENT_USER() AS __changed_by,\n","            0 as `__moved`, \n","            0 as  `__rowMarker__`\n","    FROM `{db_name}`.`{table_name}`\n","    WHERE 1=0;\n","    \"\"\"\n","    print(\"Creating CDC table:\\n\", create_cdc_sql)\n","    cursor.execute(create_cdc_sql)\n","\n","    # Drop triggers if they exist\n","    for action in [\"insert\", \"update\", \"delete\"]:\n","        trigger_name = f\"trg_{table_name}_{action}\"\n","        drop_sql = f\"DROP TRIGGER IF EXISTS `{db_name}`.`{trigger_name}`;\"\n","        print(\"Dropping trigger (if exists):\", trigger_name)\n","        cursor.execute(drop_sql)\n","\n","    # Insert trigger\n","    insert_trigger = f\"\"\"\n","    CREATE TRIGGER `trg_{table_name}_insert` AFTER INSERT ON `{db_name}`.`{table_name}`\n","    FOR EACH ROW\n","    INSERT INTO `{db_name}`.`{cdc_table}` ({full_col_list})\n","    VALUES ({col_list_new}, 'INSERT', CURRENT_TIMESTAMP, CURRENT_USER() , 0, 0 );\n","    \"\"\"\n","    print(\"Creating insert trigger:\\n\", insert_trigger)\n","    cursor.execute(insert_trigger)\n","\n","    # Update trigger\n","    update_trigger = f\"\"\"\n","    CREATE TRIGGER `trg_{table_name}_update` AFTER UPDATE ON `{db_name}`.`{table_name}`\n","    FOR EACH ROW\n","    INSERT INTO `{db_name}`.`{cdc_table}` ({full_col_list})\n","    VALUES ({col_list_new}, 'UPDATE', CURRENT_TIMESTAMP, CURRENT_USER(), 0 , 1);\n","    \"\"\"\n","    print(\"Creating update trigger:\\n\", update_trigger)\n","    cursor.execute(update_trigger)\n","\n","    # Delete trigger\n","    delete_trigger = f\"\"\"\n","    CREATE TRIGGER `trg_{table_name}_delete` AFTER DELETE ON `{db_name}`.`{table_name}`\n","    FOR EACH ROW\n","    INSERT INTO `{db_name}`.`{cdc_table}` ({full_col_list})\n","    VALUES ({col_list_old}, 'DELETE', CURRENT_TIMESTAMP, CURRENT_USER(), 0 , 2);\n","    \"\"\"\n","    print(\"Creating delete trigger:\\n\", delete_trigger)\n","    cursor.execute(delete_trigger)\n","\n","    conn.commit()\n","    cursor.close()\n","    print(f\"✅ CDC setup complete for `{db_name}`.`{table_name}`\")\n","    conn.close()\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":25,"statement_ids":[25],"state":"finished","livy_statement_state":"available","session_id":"a898c9c3-8fcd-45f0-a6c2-aabc4c689708","normalized_state":"finished","queued_time":"2025-07-19T15:51:18.5852013Z","session_start_time":null,"execution_start_time":"2025-07-19T15:51:18.5866491Z","execution_finish_time":"2025-07-19T15:51:19.0455861Z","parent_msg_id":"ad42e23c-9759-4633-9ac8-7dd7821645a6"},"text/plain":"StatementMeta(, a898c9c3-8fcd-45f0-a6c2-aabc4c689708, 25, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4146f5e3-7821-4d98-a33b-dca0ae7c6e9a"},{"cell_type":"code","source":["import mysql.connector\n","import pandas as pd\n","\n","# Exporting the whole table/file to delta\n","def export_snapshot_to_parquet(\n","    host, user, password, database, source_table, output_file=\"cdc_output.parquet\"\n","):\n","    # Connect to MySQL\n","    conn = mysql.connector.connect(\n","        host=host,\n","        user=user,\n","        password=password,\n","        database=database\n","    )\n","\n","    # Determine CDC table name\n","    cdc_table = f\"{source_table}\"\n","\n","    # Load CDC data into a DataFrame\n","    query = f\"SELECT * FROM `{cdc_table}`\"\n","    df = pd.read_sql_query(query, conn)\n","\n","    # Show the number of changes exported\n","    print(f\"Exporting {len(df)} rows from `{cdc_table}` to {output_file}\")\n","\n","    # Save as Parquet\n","    df.to_parquet(output_file, engine=\"pyarrow\", index=False)\n","\n","    conn.close()\n","    print(\"✅ Export complete.\")\n","\n","\n","def export_cdc_to_parquet(\n","    host, user, password, database, source_table, output_file=\"cdc_output.parquet\"\n","):\n","    # Connect to MySQL\n","    try:\n","        # Establish the connection\n","        connection = mysql.connector.connect(\n","            host=host,\n","            user=user,\n","            password=password,\n","            database=database\n","        )\n","\n","        if connection.is_connected():\n","            cursor = connection.cursor()\n","\n","            # Determine CDC table name\n","            cdc_table = f\"{source_table}_cdc\"\n","\n","            # Load CDC data into a DataFrame\n","            query = f\"SELECT * FROM `{cdc_table}` where __moved=0\"\n","            df = pd.read_sql_query(query, connection)\n","\n","            # Show the number of changes exported\n","            print(f\"Exporting {len(df)} rows from `{cdc_table}` to {output_file}\")\n","\n","            # Save as Parquet\n","            df.to_parquet(output_file, engine=\"pyarrow\", index=False)\n","\n","            # Define the UPDATE query\n","            update_query = f\"\"\"\n","            UPDATE {cdc_table}\n","            SET __moved = 1\n","            WHERE __moved = 0\n","            \"\"\"\n","\n","            # Execute the query\n","            cursor.execute(update_query)\n","\n","            # Commit the changes\n","            connection.commit()\n","\n","            print(f\"{cursor.rowcount} record(s) updated successfully.\")\n","\n","    except Error as e:\n","        print(f\"Error: {e}\")\n","\n","    finally:\n","        # Close the connection\n","        if connection.is_connected():\n","            cursor.close()\n","            connection.close()\n","            print(\"MySQL connection is closed.\")\n","    "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":35,"statement_ids":[35],"state":"finished","livy_statement_state":"available","session_id":"a898c9c3-8fcd-45f0-a6c2-aabc4c689708","normalized_state":"finished","queued_time":"2025-07-19T16:05:18.1849394Z","session_start_time":null,"execution_start_time":"2025-07-19T16:05:18.1863035Z","execution_finish_time":"2025-07-19T16:05:18.4314069Z","parent_msg_id":"49728394-e867-423f-99d4-5cc26aa507db"},"text/plain":"StatementMeta(, a898c9c3-8fcd-45f0-a6c2-aabc4c689708, 35, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"897c270f-41b3-42fa-a2b0-166d3efe95ac"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n","# Licensed under the MIT License.\n","\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import ClientSecretCredential\n","import requests\n","import json\n","import os\n","\n","class OpenMirroringClient:\n","    def __init__(self, client_id: str, client_secret: str, client_tenant: str, host: str):\n","        self.client_id = client_id\n","        self.client_secret = client_secret\n","        self.client_tenant = client_tenant\n","        self.host = self._normalize_path(host)\n","        self.service_client = self._create_service_client()\n","\n","    def _normalize_path(self, path: str) -> str:\n","        \"\"\"\n","        Normalizes the given path by removing the 'LandingZone' segment if it ends with it.\n","\n","        :param path: The original path.\n","        :return: The normalized path.\n","        \"\"\"\n","        if path.endswith(\"LandingZone\"):\n","            # Remove the 'LandingZone' segment\n","            return path[:path.rfind(\"/LandingZone\")]\n","        elif path.endswith(\"LandingZone/\"):\n","            # Remove the 'LandingZone/' segment\n","            return path[:path.rfind(\"/LandingZone/\")]\n","        return path\n","\n","    def _create_service_client(self):\n","        \"\"\"Creates and returns a DataLakeServiceClient.\"\"\"\n","        try:\n","            credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","            return DataLakeServiceClient(account_url=self.host, credential=credential)\n","        except Exception as e:\n","            raise Exception(f\"Failed to create DataLakeServiceClient: {e}\")\n","\n","    def create_table(self, schema_name: str = None, table_name: str = \"\", key_cols: list = []):\n","        \"\"\"\n","        Creates a folder in OneLake storage and a _metadata.json file inside it.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param key_cols: List of key column names.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Create the folder\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","            directory_client.create_directory()\n","\n","            # Create the _metadata.json file\n","            metadata_content = {\"keyColumns\": [f'{col}' for col in key_cols]}\n","            metadata_file_path = os.path.join(folder_path, \"_metadata.json\")\n","            file_client = directory_client.create_file(\"_metadata.json\")\n","            file_client.append_data(data=json.dumps(metadata_content), offset=0, length=len(json.dumps(metadata_content)))\n","            file_client.flush_data(len(json.dumps(metadata_content)))\n","\n","            print(f\"Folder and _metadata.json created successfully at: {folder_path}\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to create table: {e}\")\n","\n","    def remove_table(self, schema_name: str = None, table_name: str = \"\", remove_schema_folder: bool = False):\n","        \"\"\"\n","        Deletes a folder in the OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param remove_schema_folder: If True, removes the schema folder as well.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                print(f\"Warning: Folder '{folder_path}' not found.\")\n","                return\n","\n","            # Delete the folder\n","            directory_client.delete_directory()\n","            print(f\"Folder '{folder_path}' deleted successfully.\")\n","\n","            # Check if schema folder exists\n","            if remove_schema_folder and schema_name:\n","                schema_folder_path = f\"{schema_name}.schema\"\n","                schema_directory_client = file_system_client.get_directory_client(schema_folder_path)\n","                if schema_directory_client.exists():\n","                    schema_directory_client.delete_directory()\n","                    print(f\"Schema folder '{schema_folder_path}' deleted successfully.\")\n","                else:\n","                    print(f\"Warning: Schema folder '{schema_folder_path}' not found.\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to delete table: {e}\")\n","\n","    def get_next_file_name(self, schema_name: str = None, table_name: str = \"\") -> str:\n","        \"\"\"\n","        Finds the next file name for a folder in OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :return: The next file name padded to 20 digits.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"LandingZone/{schema_name}.schema/{table_name}\" if schema_name else f\"LandingZone/{table_name}\"\n","\n","        try:\n","            # Get the system client\n","            file_system_client = self.service_client.get_file_system_client(file_system=folder_path)\n","\n","            # List all files in the folder\n","            file_list = file_system_client.get_paths(recursive=False)\n","            parquet_files = []\n","\n","            for file in file_list:\n","                file_name = os.path.basename(file.name)\n","                if not file.is_directory and file_name.endswith(\".parquet\") and not file_name.startswith(\"_\"):\n","                    # Validate the file name pattern\n","                    if not file_name[:-8].isdigit() or len(file_name[:-8]) != 20:  # Exclude \".parquet\"\n","                        raise ValueError(f\"Invalid file name pattern: {file_name}\")\n","                    parquet_files.append(int(file_name[:-8]))\n","\n","            # Determine the next file name\n","            if parquet_files:\n","                next_file_number = max(parquet_files) + 1\n","            else:\n","                next_file_number = 1\n","\n","            # Return the next file name padded to 20 digits\n","            return f\"{next_file_number:020}.parquet\"\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to get next file name: {e}\")\n","\n","    def upload_data_file(self, schema_name: str = None, table_name: str = \"\", local_file_path: str = \"\"):\n","        \"\"\"\n","        Uploads a file to OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param local_file_path: Path to the local file to be uploaded.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","        if not local_file_path or not os.path.isfile(local_file_path):\n","            raise ValueError(\"Invalid local file path.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                raise FileNotFoundError(f\"Folder '{folder_path}' not found.\")\n","\n","            # Get the next file name\n","            next_file_name = self.get_next_file_name(schema_name, table_name)\n","\n","            # Add an underscore to the file name for temporary upload\n","            temp_file_name = f\"_{next_file_name}\"\n","\n","            # Upload the file\n","            file_client = directory_client.create_file(temp_file_name)\n","            with open(local_file_path, \"rb\") as file_data:\n","                file_contents = file_data.read()\n","                file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n","                file_client.flush_data(len(file_contents))\n","\n","            print(f\"File uploaded successfully as '{temp_file_name}'.\")\n","            \n","            # Python SDK doesn't handle rename properly for onelake, using REST API to rename the file instead\n","            self.rename_file_via_rest_api(f\"LandingZone/{folder_path}\", temp_file_name, next_file_name)\n","            print(f\"File renamed successfully to '{next_file_name}'.\")\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to upload data file: {e}\")\n","        \n","    def rename_file_via_rest_api(self, folder_path: str, old_file_name: str, new_file_name: str):\n","        # Create a ClientSecretCredential\n","        credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","        # Get a token\n","        token = credential.get_token(\"https://storage.azure.com/.default\").token\n","\n","        # Construct the rename URL\n","        rename_url = f\"{self.host}/{folder_path}/{new_file_name}\"\n","\n","        # Construct the source path\n","        source_path = f\"{self.host}/{folder_path}/{old_file_name}\"\n","\n","        # Set the headers\n","        headers = {\n","            \"Authorization\": f\"Bearer {token}\",\n","            \"x-ms-rename-source\": source_path,\n","            \"x-ms-version\": \"2020-06-12\"\n","        }\n","\n","        # Send the rename request\n","        response = requests.put(rename_url, headers=headers)\n","\n","        if response.status_code in [200, 201]:\n","            print(f\"File renamed from {old_file_name} to {new_file_name} successfully.\")\n","        else:\n","            print(f\"Failed to rename file. Status code: {response.status_code}, Error: {response.text}\")\n","\n","    def get_mirrored_database_status(self):\n","        \"\"\"\n","        Retrieves and displays the status of the mirrored database from Monitoring/replicator.json.\n","\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"replicator.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","            print(json.dumps(status_json, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","    def get_table_status(self, schema_name: str = None, table_name: str = None):\n","        \"\"\"\n","        Retrieves and displays the status of tables from Monitoring/table.json.\n","\n","        :param schema_name: Optional schema name to filter.\n","        :param table_name: Optional table name to filter.\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"tables.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","\n","            # Treat None as empty string for filtering\n","            schema_name = schema_name or \"\"\n","            table_name = table_name or \"\"\n","\n","            if not schema_name and not table_name:\n","                # Show the whole JSON content\n","                print(json.dumps(status_json, indent=4))\n","            else:\n","                # Filter tables array\n","                filtered_tables = [\n","                    t for t in status_json.get(\"tables\", [])\n","                    if t.get(\"sourceSchemaName\", \"\") == schema_name and t.get(\"sourceTableName\", \"\") == table_name\n","                ]\n","                print(json.dumps({\"tables\": filtered_tables}, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":37,"statement_ids":[37],"state":"finished","livy_statement_state":"available","session_id":"a898c9c3-8fcd-45f0-a6c2-aabc4c689708","normalized_state":"finished","queued_time":"2025-07-19T16:07:19.4081849Z","session_start_time":null,"execution_start_time":"2025-07-19T16:07:19.4098387Z","execution_finish_time":"2025-07-19T16:07:21.7506013Z","parent_msg_id":"43c418c0-a9fc-4523-bf0a-8b7bcf5735f3"},"text/plain":"StatementMeta(, a898c9c3-8fcd-45f0-a6c2-aabc4c689708, 37, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"25ea5de9-c4ca-41a8-9805-4562bb81d81d"},{"cell_type":"code","source":["\n","my_client_id=\"<Service Principal Id>\"\n","my_client_secret=\"<Secret>\"\n","my_client_tenant=\"<Azure Tenant Id>\"\n","\n","landing_zone = \"https://onelake.dfs.fabric.microsoft.com/<Workspace Id>/<Mirrored Database Id>/Files/LandingZone\"\n","\n","# Enable CDC (tiggers on a mySQL Table)\n","\n","# Setup variables\n","host='mysqlserver.domain.com'\n","user='mysql_user'\n","password='mysql_password'\n","database='mysql_database'\n","table='employees'\n","output_file=\"/lakehouse/default/Files/employees_cdc.parquet\"  \n","\n","\n","mirroring_schema = \"mysql\"\n","mirroring_table = \"employees\"\n","# setup_cdc_for_table(host,user,password,database,table)\n","\n","# Setup the OpenMirroring Client\n","client = OpenMirroringClient(\n","    client_id=my_client_id,\n","    client_secret=my_client_secret,\n","    client_tenant=my_client_tenant,\n","    host=landing_zone\n",")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":42,"statement_ids":[42],"state":"finished","livy_statement_state":"available","session_id":"a898c9c3-8fcd-45f0-a6c2-aabc4c689708","normalized_state":"finished","queued_time":"2025-07-19T16:26:28.4522277Z","session_start_time":null,"execution_start_time":"2025-07-19T16:26:28.4534712Z","execution_finish_time":"2025-07-19T16:26:31.7727926Z","parent_msg_id":"46174f4c-ae4d-4415-a7cb-111735d5dc57"},"text/plain":"StatementMeta(, a898c9c3-8fcd-45f0-a6c2-aabc4c689708, 42, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipykernel_6041/2956062065.py:54: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  df = pd.read_sql_query(query, connection)\n"]},{"output_type":"stream","name":"stdout","text":["Exporting 3 rows from `employees_cdc` to /lakehouse/default/Files/employees_cdc.parquet\n3 record(s) updated successfully.\nMySQL connection is closed.\nFile uploaded successfully as '_00000000000000000002.parquet'.\n"]},{"output_type":"stream","name":"stdout","text":["File renamed from _00000000000000000002.parquet to 00000000000000000002.parquet successfully.\nFile renamed successfully to '00000000000000000002.parquet'.\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eee79ea1-2a14-4435-93e4-89f64dc918c3"},{"cell_type":"code","source":["# remove the table from mirroring i.e. start again\n","client.remove_table(schema_name=mirroring_schema, table_name=mirroring_table)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1a59e5b6-f4d9-41a6-b004-7e6bfd7c6212"},{"cell_type":"code","source":["# Create the Mirrored table\n","client.create_table(schema_name=mirroring_schema, table_name=mirroring_table, key_cols=[\"id\"])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6dea2fad-75b4-4d6d-a5ee-c715189c7703"},{"cell_type":"code","source":["# Export snapshot\n","export_snapshot_to_parquet(host,user,password,database,table,output_file)       \n","client.upload_data_file(schema_name=mirroring_schema, table_name=mirroring_table, local_file_path=output_file)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dced631b-a147-4ea7-ab00-961f4233ab92"},{"cell_type":"code","source":["# Export some changes\n","export_cdc_to_parquet( host,user,password,database,table,output_file) \n","client.upload_data_file(schema_name=mirroring_schema, table_name=mirroring_table, local_file_path=output_file)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0b3162ce-c3f4-47bb-86be-01c950bdf3a4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"0ef2c045-1247-44ad-b722-fe2b46525e02","known_lakehouses":[{"id":"0ef2c045-1247-44ad-b722-fe2b46525e02"}],"default_lakehouse_name":"lakeexcel","default_lakehouse_workspace_id":"3e610b5b-7da7-42fa-b78f-faef8b8affe4"}}},"nbformat":4,"nbformat_minor":5}